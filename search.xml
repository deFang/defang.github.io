<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[如何做逆向投资]]></title>
    <url>%2F2019%2F191209-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E9%80%86%E5%90%91%E6%8A%95%E8%B5%84%2F</url>
    <content type="text"><![CDATA[本文是《投资中最简单的事》第一篇读书笔记。 近期在读各类价值投资大师的书，股票市场每天涨涨跌跌，看似赚钱机会到处都有，但是从来没有人靠技术分析能持续赚钱的，本质上还是因为短期来看，股市涨跌与公司本身可以无关，这时候预测股价就和预测双色球一样，但长期来看，股票价格和公司一定有关，这时候就有了一定的可预测性，也因为这一个性质，价值投资是唯一一个可以成功指导股市投资的大类方法，这一领域也在持续不断的产生投资大师。 最近在读这本邱国鹭的经典价投著作，里面的很多的话值得细细咀嚼，而且常读常新，所以分主题对其进行分享和讨论。 价值投资者对一只股票的成功投资离不开两点：好公司和好价格。一般的事件和宏观因素并不足以让一个好公司的股票产生非常大的回撤，往往只有当公司或者整个行业陷入到逆境中才会产生巨大的下跌。这时候价值投资如果做出正确的逆向投资，会产生非常丰厚的回报。 逆向投资是最简单也最不容易学习的投资方式，因为它不是一种技能，而是一种品格——品格是无法学习的，只能通过实践慢慢磨炼出来。投资领域的集大成者大多数都具有超强的逆向思维能力，尽管他们对此的表述各不相同。乔治·索罗斯说：“凡事总有盛极而衰的时候，大好之后便是大坏。”约翰·邓普顿说：“要做拍卖会上唯一的出价者。”查理·芒格说：“倒过来想，一定要倒过来想。”卡尔·伊坎说：“买别人不买的东西，在没人买的时候买。”巴菲特说：“别人恐惧时我贪婪，别人贪婪时我恐惧。” 首先，看估值是否够低、是否已经过度反映了可能的坏消息。估值高的股票本身估值下调的空间大，加上这类股票的未来增长预期同样存在巨大下调空间，因此这种“戴维斯双杀”导致的下跌一般持续时间长而且幅度大，刚开始暴跌时不宜逆向投资。2011—2012年，A股计算机行业的许多“大众情人”在估值和预期利润双双腰斩的背景下持续下跌了70%就是例证。2012年年底，这些股票从成长股跌成了价值股，反而可以开始研究了。 其次，看遭遇的问题是否是短期问题、是否是可解决的问题。例如，零售股面临的网购冲击、新建城市综合体导致旧有商圈优势丧失、租金劳动力成本上涨压缩利润空间等问题就不是短期能够解决的，因此其股价持续两年的大幅调整也是顺理成章的。不过，现在大家都把零售当作夕阳行业，反而有阶段性反弹的可能——尽管大的趋势仍然是长期向下的。 最后，看股价暴跌本身是否会导致公司的基本面进一步恶化，即是否有索罗斯所说的反身性。贝尔斯登和雷曼的股价下跌直接引发了债券评级的下降以及交易对手追加保证金的要求，这种负反馈带来的连锁反应就不适合逆向投资。中国的银行业因为有政府的隐性担保（中央经济工作会议指出“坚决守住不发生系统性和区域性金融风险的底线”），不存在这种反身性，因此可以逆向投资。 逆向投资最让人不爽的一点是，往往是抄底抄在半山腰。这对于投资者是非常难以掌控的，因为合格的投资者只知道一只股票最终会涨，但不知道什么时候涨，类似的，他也知道这只股票最终会止跌，但不知道什么时候止跌。目前市场上比较常见做法是在下跌阶段分批次买入法，如果在没有全部买完时，股票就已经开始反弹，那就作罢，接受只赚这么多。这其实是接受了自己没有择时能力的最优解，就像大宗交易用twap来减少交易成本一样。 有些股票，你有持仓，但是下跌时你心里一点也不慌，甚至希望它多跌一点好让你加仓，这说明你对该股票已有足够了解，对其内在价值和未来前景有比市场更为精准的把握，因此市场价格的波动已经不会影响到你的情绪了。对这些股票而言，下跌只是提供一个更好的买点罢了——买之后的淡定，源自买之前的分析。2012年的白酒股因为塑化剂事件大幅跳水，在面对其他类似食品安全事件的逆向投资机会时，投资者可以思考这样几个问题： • 有无替代品，若有替代品（例如三株口服液之类的营养品就有众多替代品），则谨慎，若无替代品，则积极； • 是个股问题还是行业问题，如果主要是个股问题，则避开涉事个股，重点研究其竞争对手，即使是行业问题（例如毒奶粉），也可关注受影响相对较小的个股； • 是主动添加违规成分还是“被动中枪”，前者宜谨慎，后者可积极； • 该问题是否容易解决，若容易解决，则积极，若难以解决（例如三聚氰胺问题），影响可能持续的时间长且有再次爆发的可能性，则谨慎； • 涉事企业是否有扎实的根基，悠久的历史传承和广泛的品牌美誉度在危机时刻往往有决定性的作用，秦池、孔府的倒台就是由于根基不稳而盘子却铺得太大； • 是否有突出的受害者个例，这决定了事件对消费者的影响是否持久。 BREAKING NEWS是机会，人们总是对爆炸性新闻关注过多，却对大历史趋势习以为常，在美国911事件后买入航空股的人都获利颇丰，7·23”甬温线特别重大铁路交通事故后的一两个月购买铁路建设和铁路设备的股票的投资者，也大幅跑赢市场一年多。 其实还有新城控股最近的猥亵案，如果能够在最低点买入，已经能够赚50%了。但同样，这些逆向机会非常容易出现抄底抄在半山腰的情况。对于这一点，我并没有好的解决方法。 其实，独立思考、逆向而动效果往往更好。基金公司作为一个整体的行业配置，在一般情况下是对的（毕竟专业人士相较于其他市场参与者还是有一定优势的），但是在极端的情况下，基金公司也很可能是错的。2014年年初，在基金公司的行业配置中，对TMT（Technology，Media，Telecom，科技、媒体和通信产业）和医药的超配程度以及对金融地产的低配程度都达到了十年之最。上一次基金整体配置如此失衡是在2010年年底。2010年11月我接受《中国证券报》采访时，提到的一个论题就是“银行与医药股票哪一个前景更好”，当时我的一个基本结论就是医药比银行贵3倍，但是增长不可能比银行快3倍。在之后的两年中，2011年银行股跌了5%，医药股跌了30%，2012年银行股涨了13%，医药股涨了6%，两年累计下来看，机构一致低配的银行股大幅跑赢了机构一致超配的医药股，再一次验证了“最一致的时候就是最危险的时候”这句老话。2013年各机构再次一致地憧憬着老龄化对医药的无限需求，把医药股的估值推高到30倍市盈率。比起5倍市盈率的银行，当时机构做出的比较和得出的基本结论现在几乎可以原封不动地重复一遍。 人多的地方不去，目前A股正在吹捧核心资产，上周看到一个基金经理大肆鼓吹中证100就是核心资产，其中的核心理论依据就是中证100近两年比沪深300，中证500涨的好。正所谓涨了的就是核心资产，不涨就不是。邱国鹭对于这种情况，已经给出了解决方法：坦率地说，我也不知道医药股的高估值还能持续多久，也许会从高估变成更高估。不论错误定价的程度有多大，没有人能够事前预知拐点。作为投资者，我们能分辨清楚的就是市场的错误定价在哪个板块以及错误的程度有多大，然后远离被高估的板块，买入被低估的公司。至于市场要等多久才会进行纠错，纠错前会不会把这种错误定价进一步扩大，就不是能够预测的了。 当然，任何投资方法都有缺陷，逆向投资的短板就是经常会买早了或者卖早了。买早了还得熬得住，这是逆向投资者的必备素质。投资者必须明白一个道理，市场中没有人能够卖在最高点、买在最低点。在2007年的牛市中，即使指数后来涨到了6 100点，能够在4 000点以上出货也是幸运的；在2008年的熊市中，即使指数后来又跌到了1 664点，能够在2 000点建仓也是幸运的。顶部和底部只是一个区域，该逆向时就不要犹豫，不要在乎短期最后一跌的得失，只要能笑到最后，短期难熬点又何妨？只有熬得住的投资者才适合做逆向投资。在A股这样急功近利的市场中，能熬、愿熬的人少了，因此逆向投资在未来仍将是超额收益的重要来源。```]]></content>
      <tags>
        <tag>投资</tag>
        <tag>读书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈谈A股唯一一家生活用纸企业]]></title>
    <url>%2F2019%2F191009-%E4%B8%AD%E9%A1%BA%E6%B4%81%E6%9F%94%2F</url>
    <content type="text"><![CDATA[中顺洁柔是一家生活用纸企业，也是A股唯一一家上市的生活用纸企业，产品类型包括我们日常常见卷纸、抽纸、纸手帕、湿巾等，其主打品牌为”洁柔”。 基本情况目前中顺营业收入在60亿元左右，毛利率35%，其中营业成本的60%是纸浆成本，所以中顺理论上应该有周期股的特性，特别是2017下半年国际纸浆价格上涨了50%，2018也维持高位，但中顺依然维持了稳定的增长，可见其管理层的优秀功力。 中顺洁柔的业务非常纯粹，根据2018年报，其营业收入的98%来自于生活用纸，而A股的纸业公司普遍存在多元化经营，业务线庞杂等现象，这也凸显出中顺洁柔管理层的独特。 通过整理对比A股的造纸行业股，中顺洁柔的毛利率在整个行业中近三年一直稳居第一，基本维持在35%左右，但是净资产收益率能力差不多在12%，在整个行业中并不算最出色的。主要是因为，作为TO C的生活用纸企业，中顺的销售费用率一直在10%左右，相比很多TO B的造纸企业，显然不占优势。 中顺近三年企业经营情况表现稳健，营收每年增长20%，净利润每年增长30%，这一方面是消费股的优势，另一方面也再次说明中顺管理层的优秀，在2018以来宏观环境不佳的背景下，还能逆势取得稳定增长，而且净利润增长比营收增长快，说明各项成本的得到加速控制。 行业对比目前，国内生活用纸行业包括了恒安国际、金红叶纸业、维达国际和中顺洁柔，其中恒安国际和维达国际是港股上市企业。 恒安国际是行业第一，在经营策略上走的是多元化集团的路线，旗下有纸巾产品（中顺的直接竞品）、卫生巾产品、护肤产品和纸尿裤产品，在产品定位上横跨高中低各档产品线，收入合计超200亿，其中纸巾产品占比50%。 金红叶为金光纸业在中国的生活用纸运营主体，经营策略上的特色是纵向一体化（集团自供应原材料），于此带来的是整体上的成本优势，顺着这种优势，经营定位上是主打中低端市场+大单品策略，其中“清风”系列产品毛利润占比多年保持在总量的80%以上。 维达国际2018年营业收入合计大概在150亿左右，其中生活用纸产品占比80%，其经营策略和产品定位和中顺非常类似，因此我认为是中顺在市场上最直接的竞争对手。目前中顺年营业收入在60亿左右，基本上是维达一半的体量，但从增长率来看，这三家企业的生活用纸单项增速都在10%以下，只有中顺维持20%以上的收入增速。 估值分析中顺目前的增长和盈利已经受到市场的认可，其PETTM已经摸到了历史较高水位，达到37。同类型的维达国际在港股的市盈率为24，恒安国际市盈率为15。以目前的市盈率来看，中顺已经不便宜了，即使它是这个行业中增长速度最快的企业，其增长潜力也很大，但是增长绝对速度也只有20%，并不算是高速增长，目前市场的估值有些偏乐观了。 中顺其实还有一个隐忧。一方面消费者对生活用纸的价格是比较敏感的，企业一般很难单独做出提价，因为消费者对于品牌并没有很强的认知，依然是被价格主导。虽然中顺努力开拓品类，开发湿巾类，婴儿用纸等高利润产品，但毕竟其收入占比目前仍然较低。 另一方面，其实中国仍然有大量的四五线城镇的人，平时是不怎么用抽纸/面巾纸的。据统计，我国人均生活用纸消费量达到6.2kg，与发达地区相比，日本韩国 23-25 公斤、欧美 30 公斤、香港/台湾地区 10+公斤有显著差距。从局部情况来看，北京、上海等一线城市人均生活用纸消费量已经达到10kg，已经追上港澳地区，但四五线城市的需求显然还没有被挖掘出来。 但目前这一情况在迅速改善，但是很可惜和中顺并没有关系。不知道大家知不知道“丝飘“这个品牌？它是一个完全依靠拼多多起家的纸巾品牌，依靠29.9元买30包抽纸的爆款，目前年销售额超过4亿元，因为其不到1元一包的单价，还被动出口到越南。这些企业本身是OEM,ODM出身，在外贸订单萎缩背景下，依靠拼多多的新品牌扶持计划，迅速占领广大低线城市市场，甚至是挖掘出原来大企业不曾培育出的新市场。这些企业的产品定价非常接近成本，利润率相当之低，但只要他们把规模做上去，仍然能活的很好。所以在这个游戏里，消费者得到质优价廉的商品，工厂活了下来，拼多多也得到用户，损失的只有类似中顺的传统大企业，它丧失了获取这块市场的机会。 所以中顺依靠优秀的管理能力，相信仍然能维持稳定的增长，但远期的困境可能就在于向上突破还有点远，向下防守力不从心，结合目前的估值，可能并不是最好的入手时机。]]></content>
      <tags>
        <tag>投资</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈谈值得买（300785）-这家细分行业龙头还值得买吗]]></title>
    <url>%2F2019%2F191008-%E5%80%BC%E5%BE%97%E4%B9%B0%2F</url>
    <content type="text"><![CDATA[今天要谈的公司是值得买（300785）。值得买这家公司大家应该都比较熟悉，在A股这个群魔乱舞的市场，大家还是比较认可值得买，一方面是作为一家月活超2000万的电商导购平台，很多股民也是值得买的用户，况且值得买的定位就是一二线城市的男性用户，与A股投资者有天然的契合；另一方面，值得买的主营业务非常干净纯粹，就是给电商导流，暂时还没有染上A股市场一些垃圾公司喜欢搞资产重组，概念炒作的坏毛病，这点深得价投喜爱。 值得买主要产品就是smzdm.com网站和APP，公司前身是公司创始人隋国栋搭建出个人网站“smzdm.com”，为消费者收集各平台的优惠信息，筛选折扣力度最大的促销信息并通过网站发布给消费者，其收入来源主要是靠电商返佣和广告。作为互联网企业，其资产负债结构也相对简单，所以我们主要看下其利润表中的营收情况。 再加上一些基本的财务指标 初看这张利润表，我会觉得这是一家非常会赚钱的企业，毛利率在70%以上，净利率20%左右，营收增长率30%以上，所在的赛道（内容导购平台）在A股也没有竞争对手，具有稀缺性，感觉是A股少有的兼具好公司好行业的标的。 看的再仔细一点，感觉有些地方也不是那么完美：为什么2018年之后营业收入增长率从80%下降到38%，但营业成本反而加速上涨，毛利率也从83%下降到73%，毕竟是市盈率60倍的股，盈利能力下降可不是什么好消息，感觉这里面会有坑。 为了解答这个的问题，我仔细阅读了公司的招股书和最新的中报，果然，现实总是残酷的，让我一一道来。 我们首先分解报表上的营业收入，值得买将营业收入分为两类，一类是信息推广业务，其中按照收费模式的不同，分为电商导购佣金收入和广告展示收入。另一类是互联网效果营销平台服务，简单来说就是值得买成立了一个子公司做中间平台，一端连接拥有流量的网站、移动客户端和内容创业者等媒体，一端连接电商、品牌商广告主。但目前这个中间平台卖的流量主要都是值得买自己的网站和APP资源，所以本质上这部分收入和电商导购佣金性质是一样的。 所以从业务属性上来看，信息推广业务中的电商导购收入和互联网效果营销收入赚的是佣金，也就是按照CPS对商家收费，我把它叫做佣金收入，佣金收入=佣金费率GMV；而信息推广业务中的广告展示收入是按照广告位刊例价收费，一般是几万元/天，这里简称为广告收入，广告收入=刊例价广告位数量。 值得买广告刊例价（来源：招股书） 根据招股书披露的数据，我计算了各年度佣金收入和广告收入，发现基本上值得买至少一半收入是来源于广告，例如2018年的5.06亿收入中，广告收入为3.06亿，佣金收入为2亿元。 在2017，广告收入同比大幅增长了125%（来自于广告刊例价大幅提升），但在2018广告增速回落到35%（刊例价保持稳定），这直接导致营业收入增速从2017年的82%回落至38%。相比而言，佣金收入增速一直比较稳定，维持在40%左右。由于公司网站和APP端的广告位数量已不可能大幅增加，在宏观经济不景气的背景下，刊例价也没有大幅提升的可能，未来广告收入增速不太可能回到之前的超高速增长了。我预估广告收入增长会小于佣金增长，这会导致营收承压，未来增长率可能会小于30%。 通过上面的计算，目前值得买的广告收入占营收比重已达到60%，感觉值得买已经变成一个媒体公司了？这一点可能和广大用户的认知会有冲突，相信有很多张大妈的忠实用户都认为值得买是一家比较中立的消费信息聚合平台，但是当网站和APP上充斥了各种广告，这其实是很影响用户体验的。 从我个人的用户体验出发，其实现在smzdm.com上已经充满了广告，但因为它并没有像搜索引擎那样在banner下方标明这是广告，而是把自己伪装成优惠信息，很多人根本看不出来这是广告。这必然对平台的公信力和中立度有损害，别忘了值得买的用户群是理性思维能力最强的理工男，进一步提高广告收入占比损害的是平台长远利益，只有依靠挖掘优质信息，提高GMV带动佣金收入，才是平台健康的内生生长路径。 所以收入放缓的原因我们找到了，但另一个不好的现象是，营业成本依然在狂飙。2018年营业成本增长了127%，2019年中报依然增长88%，这说明新获取收入的成本越来越贵，导致其毛利率从83%下滑至72%。公司招股书对营业成本增长的解释是，用户增长和推荐内容爆炸式上升，新增了大量运营人员和IT开支。 OK，那为什么成本增长和营收增长没有匹配呢？招股书中没有解释，但我还是找到一个方法来衡量新增成本到底有没有效益。 值得买的核心优势之一就是用户贡献内容（UGC），用户将电商平台上发现好价或者原创经验发布在平台上，目前用户贡献内容占平台整体内容数量的70%，贡献收入占比也在70%左右。为了增加爆料信息时效性和覆盖面，公司在2017年上线了机器贡献（MGC），通过爬虫自动监控全网好价，然后自动生成商品描述并发布。这一块内容虽然发布条数数量多（占比17%），但是收入贡献占比很少（3.7%）。 值得买优惠内容条数和营收贡献（来源：招股书） 公司的招股书中给出了近三年值得买平台上发布的优惠内容条数和相应收入信息。基本上公司的优惠信息数量呈每年翻一番的速度增长，并且在18年由于MGC的贡献，更是增长了155%。由于用户发布内容需要大量的人工审核，因此营业成本中运营开支也产生了相应的增长，且IT费用也会成比例增加。所以2018年的营业成本增速127%，这一点和优惠信息数量增速（剔除MGC）118%是比较匹配的。 我们之前说到，2018年佣金收入只增加了46%，为什么内容数量的增加为何没有带来营收的同等大幅度增加呢？我将贡献收入/内容条数作为衡量优惠内容的带货能力的指标，这个指标实质上就是每发布一条内容能为公司赚取多少钱。 近三年，除了编辑贡献内容带货能力是稳定增长的，其余类型内容的带货能力都有不同程度下降。特别是占70%比例的用户贡献内容，从2016年的142元/条下降到2018年的64元/条。这一点从招股书披露的GMV数据也能得到验证，根据计算，每发布一条优惠内容所产生的GMV是在不断下降的：从2016年的5862元下降到2018年3445元，即使剔除掉带货能力最差的MGC，2018年的单条内容产生GMV也只有4152元。 这是一个比较严重的问题，说明UGC质量已大不如前，但带来的单位成本并没有减少，导致成本的增速远高于营收增速，另一方面，用户也对信息过载产生了疲劳，目前公司的ARPU用户平均收入是19元/年，低于腾讯（53 元/年）、爱奇艺（21 元/年）和微博（23 元/年），这个ARPU的表现不算好，且已经有未老先衰的迹象。 值得买19年中报营收是2.73亿元，按照前两季度占年收入40%计算，2019年总收入预计在6.82亿元，同比增长率34%，净利润1.22亿元，同比增长率23%，且未来三年平均增长率应该会小于20%，这已经是比较乐观的估计，显然当前的股价（市盈率60+）已经透支了未来好几年的发展。 最后，是我夹带私货的一些唠叨。值得买这个公司在国内没有完全一样的竞品，在国外其实也没有规模相若的对手（只有一家北美省钱快报产品和盈利模式类似），这是一个依附于大行业（电商）的利基市场，它能赚钱，但是有显著的天花板，所以虽然他是这个利基市场的龙头，但行业规模限制了它可能只能做到这么大，除非它拓展业务，改换赛道，但值得买又比较忌讳自己亲自下海做电商，因为它担心占自己收入来源60%+的大客户（京东+淘宝）抛弃它。未来值得买可以继续讲自己的原来的故事，只是增速必然会越来越慢，和几年前相比，现在国内有很多产品已经能部分替代值得买的产品体验，如商品评测（盖得排行），商品体验分享（知乎），高性价比平台（小米，拼多多），甚至未来这个利基市场会不会一直存在下去，或者被其他行业改变都是一个疑问。互联网行业的发展速度决定了所有在这个行业玩家必须拼命奔跑，不进则退，而值得买现在可能已经不值得买了。]]></content>
      <tags>
        <tag>投资</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[社融是什么]]></title>
    <url>%2F2019%2F190822_public-loan-component%2F</url>
    <content type="text"><![CDATA[社会融资规模(社融)指的是实体经济从金融体系拿到的钱。可以是金融机构直接给的钱，比如贷款、保险赔偿金等；也可以是金融机构帮着拉皮条搞来的钱，比如非金融企业发行股票、债券等。 虽然是中国特有的经济指标，不过社融基本上可以等价于IMF（国际货币基金组织)倡导的“信用总量”的概念。反映的是实体经济对货币的需求量。 社融统计的是境内机构和个人投资到实体经济的钱，所以不统计境外机构的投资，也不统计金融机构之间的融资，如同业拆借。 社融的组成 表内融资：人民币贷款指的是金融机构出借给非金融企业/机关团体/个人的人民币贷款(贷款合同、票据贴现、垫款、贸易融资等形式都算)。个人房贷是属于该项目下。 外币贷款和人民币贷款一样，只不过币种是外币。 人民币贷款和外币贷款都是“表内业务”，金融机构发放贷款时需要把帐记到资产负债表里头，靠利差来赚钱。 监管会时不时地来查查表，而且还要求银行需要针对这些业务上交存款准备金，还要满足资本充足率等要求 而下面的这三个则是“表外业务”。金融机构主要做的是拉皮条的活，收手续费。资产不入表，所以也没有上面提到的限制。 表外融资（非标融资）： 委托贷款A要跟B借一笔钱，利率期限什么都谈好了，但是B没有放贷资质，所以B委托银行来当个中间人。这笔放款就叫做委托贷款。 信托贷款投资者可以把钱投资到资金信托产品里头(可以简单理解为信托公司卖的基金)，信托公司会把一部分钱拿去放贷，这部分就是信托贷款。 未贴现银行承兑汇票企业A跟B做生意，A欠B钱，于是开了张欠条(汇票)给B。并且A找来了银行，银行承诺一定期限后B可以拿这张票来换钱。B为了早点拿到钱，也可以提前把这张欠条打折卖给银行(打折出售哈银行的过程披称为“贴现”)。“未贴现银行承兑汇票”这个指标指的是B还hold着、还没有打折出售的那部分汇票。 从社融存量的结构上来看的话，人民币贷款占了接近七成。 企业债券融资，非金融企业境内股票融资都比较好理解。 地方政府专项债券，该项融资主要对接公路建设、棚改等基建项目，属于向实体经济提供资金范围]]></content>
  </entry>
  <entry>
    <title><![CDATA[快速检测时间序列是否为平稳序列]]></title>
    <url>%2F2018%2F181217_%E5%BF%AB%E9%80%9F%E6%A3%80%E6%B5%8B%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%98%AF%E5%90%A6%E4%B8%BA%E5%B9%B3%E7%A8%B3%E5%BA%8F%E5%88%97%2F</url>
    <content type="text"><![CDATA[对于没有学习过随机过程课的我而言，判定一个序列是否为平稳主要靠看图说话…而且机器学习跑模型，一般而言也不需要了解特征这块的性质，但最近做量化模型比较多，如果一个特征是非平稳的话，它可能会给模型一些不好的先验，这是我想避免的。所以这篇文章主要讲一下如何快速判断一个时间序列是否是平稳的。 需要了解一个概念，一般非平稳序列都有单位根(unit root)，所以可以使用单位根检验来判定一个序列是否是平稳，最常用的检验就是A.D.Fuller检验。我把上证综指的日收益率序列画出来，基本上可以看到还是比较平稳的。1234567891011import bottleneck as bnrolmean = bn.move_mean(timeseries, window=14)rolstd = bn.move_std(timeseries, window=14)#Plot rolling statistics:orig = plt.plot(timeseries, color='blue',label='Original')mean = plt.plot(rolmean, color='red', label='Rolling Mean')std = plt.plot(rolstd, color='black', label = 'Rolling Std')plt.legend(loc='best')plt.title('Rolling Mean &amp; Standard Deviation')plt.show(block=False) 下面对这个时间序列使用AD Fuller检验，statsmodels里有现成的函数可以调用。1234567891011121314151617from statsmodels.tsa.stattools import adfullerdftest = adfuller(timeseries, autolag='AIC')dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])for key,value in dftest[4].items(): dfoutput['Critical Value (%s)'%key] = value print (dfoutput) -------------------------------------------Results of Augment Dickey-Fuller Test:Test Statistic -1.266111e+01p-value 1.304351e-23#Lags Used 1.400000e+01Number of Observations Used 3.025000e+03Critical Value (1%) -3.432514e+00Critical Value (5%) -2.862496e+00Critical Value (10%) -2.567279e+00dtype: float64 上面把AD Fuller检验结果也打印出来了。一般我就看一下p-value是否小于0.05，或者看Test Statistic是否小于*Critical Value(5%)，如果小于的话，就表示这是平稳序列。]]></content>
      <tags>
        <tag>Statistics</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Xgboost特征重要性的可视化]]></title>
    <url>%2F2018%2F181217_Xgboost%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[对于决策树模型而言，其相比神经网络一个优势就是还能保留一点的可解释性。当然对于GBDT树，除了第一颗树我们可以从决策树的角度来理解模型是怎么工作，后面的所有树其实都是在学习残差，所以也不是很容易直观理解了。但无论如何，我们总是可以很方便的统计所有树模型的特征的重要性，重要性的衡量依据可以是特征使用次数，也可以特征增益。 对于XGboost模型，一般使用这行代码把特征重要性打印出来1model.get_fscore() 如果是lightGBM模型，改为这行代码1model.feature_importance() 我最近在训练模型的时候，希望能够看到模型每一步/每几步迭代过程中的特征使用情况，而不只是在模型全部训练完之后看一个整体。所以就写了以下代码，能够把模型迭代过程中的特征使用情况以及变化趋势可视化地展示出来。 模型训练我们用sklearn提供的wine dataset作为我们的toy example.12345678from sklearn import datasetsimport pandas as pdimport numpy as npimport xgboost as xgbwine = datasets.load_wine()X = wine['data']y = wine['target'] 训练集，测试集分割，构造DMatrix12345from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)dtrain = xgb.DMatrix(data=X_train, label=y_train, feature_names=wine['feature_names'])dtest = xgb.DMatrix(data=X_test, label=y_test, feature_names=wine['feature_names']) 定义XGboost训练参数1234default_param = &#123;'booster':'gbtree', 'silent':0, 'objective':'multi:softmax', 'num_class':3, 'eta':0.1, 'max_depth':4, 'eval_metric':'mlogloss', 'tree_method':'hist'&#125;num_round = 100 因为我们需要记录模型迭代过程中特征重要性，所以需要写一个callback函数让xgboost在每一轮迭代后调用，这个函数的作用就是记录下当时模型的特征重要性。123456789def record_fscore(fscore_result): if not isinstance(fscore_result, dict): raise TypeError('fscore_result has to be a dictionary') fscore_result.clear() def call_back(env): if env.iteration % 2 == 0: #每两轮记录一次 fscore_result[env.iteration] = env.model.get_fscore() return call_back 模型训练12345fscore_result = &#123;&#125;model = xgb.train(default_param, dtrain, num_boost_round=num_round, evals=[(dtrain, 'train'), (dtest, 'test')], verbose_eval = True, callbacks = [record_fscore(fscore_result)]) 训练完成后，fscore_result是这样子的：123456789101112print (fscore_result)&#123;0: &#123;'proline': 2, 'ash': 1, 'flavanoids': 2, 'alcalinity_of_ash': 1, 'color_intensity': 2, 'alcohol': 1&#125;, 2: &#123;'proline': 6, 'ash': 1, 'flavanoids': 6,... 特征可视化特征可视化我们使用plotly,相比matplotlib的优点在于它会生成一个可交互的html页面，并且它的heatmap灵活性比matplotlib强太多了。 首先定义一个后面会用到的辅助函数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def array_map(arr, mapping, default=np.NaN, on_missing='raise', return_pandas=False): """ 把arr的值，通过给定的一个映射关系mapping，应设成mapping中的value mapping: dict/pd.Series/(key_array, value_array) default: 如果有值不存在于mapping中，fill成default on_missing: 'raise'/'fill', 默认如果有不存在的值，会抛出异常，否则用default填充 return_pandas: 如果mapping是Series或者DataFrame，是否返回Series和DataFrame（默认为False，返回ndarray） example: &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; import pandas as pd &gt;&gt;&gt; arr = np.array([1, 2, 1, 3, 2]) &gt;&gt;&gt; mapping = &#123;0:1, 1:0, 2:3, 3:2&#125; &gt;&gt;&gt; array_map(arr, mapping) array([0, 3, 0, 2, 3]) &gt;&gt;&gt; array_map(arr, pd.Series([0, 1, 2, 3], index=[1, 0, 3, 2])) array([0, 3, 0, 2, 3]) """ if isinstance(mapping, tuple): arr_key = np.array(mapping[0]) arr_value = np.array(mapping[1]) if len(arr_value.shape) == 0: arr_key, arr_value = np.broadcast_arrays(arr_key, arr_value) if on_missing == "raise": idx_mapping = pd.Series(np.arange(arr_key.shape[0], dtype=np.uint32), index=arr_key) else: arr_value = np.append(arr_value, np.expand_dims(default, axis=0), axis=0) idx_mapping = pd.Series(np.arange(arr_key.shape[0], dtype=np.uint32), index=arr_key) mapped_indices = array_map(arr, idx_mapping, default=arr_key.shape[0], on_missing=on_missing, return_pandas=False) return arr_value[mapped_indices] ks, indices = np.unique(arr, return_inverse=True) if isinstance(mapping, pd.Series) or isinstance(mapping, pd.DataFrame): if on_missing == 'raise': n_idx = np.unique(mapping.index.values) n_all = np.unique(np.concatenate([ks, mapping.index.values])) if n_all.shape[0] &gt; n_idx.shape[0]: raise Exception(f"MISSING VALUE IN MAPPING: &#123;set(n_all) - set(n_idx)&#125;") ans = mapping.reindex(ks, fill_value=default).values[indices] if return_pandas: if isinstance(mapping, pd.Series): return pd.Series(ans, index=ks[indices]) elif isinstance(mapping, pd.DataFrame): return pd.DataFrame(ans, columns=mapping.columns, index=ks[indices]) else: return ans else: if on_missing == 'raise': return np.array([mapping[k] for k in ks])[indices] else: return np.array([mapping.get(k, default) for k in ks])[indices] 开始画图了12345678910111213141516from plotly.offline import plotimport plotly.graph_objs as goif not isinstance(fscore_result, dict): raise TypeError(&apos;fscore_result should be a dictionary&apos;)feature_list = dtrain.feature_namesrounds = list(fscore_result.keys())num_round = len(fscore_result)num_feat = len(feature_list)z = np.zeros((num_round, num_feat), dtype=np.int)for i, round in enumerate(rounds): z[i,:] = array_map(feature_list, fscore_result[round], on_missing=False, default=0.0001)z_prev = np.pad(z, ((1,0),(0,0)), mode=&apos;constant&apos;, constant_values=0)[:-1]z_inc = z - z_prev z_inc 即为模型每轮新增使用的特征计数，下面就是画heatmap的代码。123456789101112data= [go.Heatmap( ​ z=z_inc,​ x=feature_list,​ y=np.arange(num_round),​ colorscale=&apos;Viridis&apos;,​ )]layout = go.Layout( title=&apos;Feature Importance&apos;)fig = go.Figure(data=data, layout=layout)plot(fig, filename=&quot;feat_importance_heatmap.html&quot;) 最终生成的html如图 相比只看一个最终整体的特征重要性，观察每轮/每几轮 模型迭代使用的特征情况还是能够带来一些新信息的，我们可以看到有些特征，一开始有些用，到后期就没有用了，而有些特征则一直都很用。]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对比T检验卡方检验和F检验]]></title>
    <url>%2F2018%2F180401_T%E6%A3%80%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[T检验，卡方检验和F检验是最常见的三大统计假设检验方法，本文将主要介绍在何种情况下，如何使用某一种检验方法，但对具体的原理不做过多解释，重在实用。 T检验T检验，针对的是样本均值。假设总体均值$\mu=1.1$已知，现有一组服从正态分布的数据样本集合： 1(1) 1.0, 1.2, 1.4, 1.1, 1.3, 1.2 能否判断这组数据样本是否来自于已知总体，限定显著性水平$\alpha=0.05$. 解： 步骤一：我们选择T统计量。T统计量的计算公式如下 $$T= \frac{\hat{x}-1.1}{s/ \sqrt{n}}$$ 其中，$\hat{x}$是样本均值，$s$为样本标准差，计算公式为$s=\sqrt{\frac{1}{n-1} \sum{i=1}^{n}(x{i}-\hat{x})^2}$ 计算得到 $T=0.28867$ 步骤二：查T检验临界值表。因为样本中拥有6份数据，因此采用$n=5$（自由度为5）所对应的行；显著性水平$\alpha=0.05$，因此采用双侧检验$p=0.05$所对应的列。 可以看到，查表值为2.571， 由于$T=0.28867&lt;2.571，所以我们可以认为样本是来自于$\mu=1.1$的总体。 t检验类型 ​ t检验有多种类型，可以分为只有一组样本的单体检验和有两组样本的双体检验。单体检验用于检验样本的分布期望是否等于某个值。双体检验用于检验两组样本的分布期望是否相等，又分为配对双体检验和非配对双体检验。配对双体检验的两组样本数据是一一对应的，而非配对双体检验的两组数据则是独立的。比如药物实验中，配对双体检验适用于观察同一组人服用药物之前和之后，非配对双体检验适用于一组服用药物而一组不服用药物。 ​ 1）单体检验​ 单体检验是针对一组样本的假设检验。零假设为$\pmb{H}_0:u=u_0$,统计量 $t = \frac{\bar{x} - \mu_0}{s/\sqrt{n}}$服从自由度$n-1$的 T 分布。 ​ 2）配对双体检验​ 配对双体检验针对配对的两组样本。配对双体检验假设两组样本之间的差值服从正态分布。如果该正态分布的期望为零，则说明这两组样本不存在显著差异。零假设为 $\pmb{H}_0:u=u_0$。统计量$t = \frac{\bar{d} - \mu_0}{s/\sqrt{n}}$服从自由度为 $n-1$ 的 T 分布，其中 $\bar{d}$ 是差值的样本均值，$s$是差值的样本标准差。 ​ 3）非配对双体检验​ 非配对双体检验针对独立的两组样本。非配对双体检验假设两组样本是从不同的正态分布采样出来的。根据两个正态分布的标准差是否相等，非配对双体检验又可以分两类。一种是分布标准差相等的情况。零假设是两组样本的分布期望相等，统计量 T 服从自由度为 $n_1+n_2-2$l的 T 分布。 $$\begin{eqnarray} t &amp;=&amp; \frac{\bar{x_1}-\bar{x2}}{s{x_1,x_2} \sqrt{1/n_1+1/n2}} \nonumber\ s{x_1,x_2} &amp;=&amp; \sqrt{ \frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2} } \nonumber \end{eqnarray}$$ 其中 $\bar{x_1}]$和 $\bar{x_2}$分别是两组样本的样本均值，$n_1$ 和$n_2$分别为两组样本的大小，$s_1$ 和 $s_2$分别是两组样本的样本标准差。另一种是分布标准差不相等的情况。零假设也是两组样本的分布期望相等，统计量 T 服从 T 分布。 $\begin{eqnarray} t = \frac{\bar{x}_1-\bar{x}_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}} \nonumber \end{eqnarray}$ T 分布的自由度为 $\begin{eqnarray} d.f. = \frac{ (s_1^2/n_1+s_2^2/n_2)^2 }{(s_1^2/n_1)^2/(n_1-1)+ (s_2^2/n_2)^2/(n_2-1)} \nonumber \end{eqnarray}$ Z检验（T检验的变体）Z检验并不能算是一种全新的统计检验方法，刚刚的T检验，我们是只知道总体的均值$\mu$,但不知道总体的方差。如果我们既知道总体的均值，也知道总体的方差$\sigma$，那此时就适用Z检验。 此时Z-score统计量公式为： $$Z= \frac{\hat{x}-1.1}{\sigma/ \sqrt{n}}$$ Z检验就是查正态分布表。因为Z-score是完全服从$N(0,1)$正态分布，事实上，当n的数量足够大时（一般&gt;45)，T检验都可以不用查表了，直接查正态分布表即可。 $\chi$检验又称Chi-Squared Test，最有名的是皮尔逊卡方检验，维基百科对此有很好的描述。 皮尔森卡方检验（英语：Pearson’s chi-squared test）是最有名卡方检验之一（其他常用的卡方检验还有叶氏连续性校正、似然比检验、一元混成检验等等－－它们的统计值之机率分配都近似于卡方分配，故称卡方检验） “皮尔森卡方检验”的虚无假设（H0）是：一个样本中已发生事件的次数分配会遵守某个特定的理论分配。 在虚无假设的句子中，“事件”必须互斥，并且所有事件总机率等于1。或者说，每个事件是类别变量（英语：categorical variable）的一种类别或级别（英语：level）。 简单的例子：常见的六面骰子，事件＝丢骰子的结果（可能是1~6任一个）属于类别变量，每一面都是此变量的一种（一个级别）结果，每种结果互斥（1不是2, 3, 4, 5, 6; 2不是1, 3, 4 …），六面的机率总和等于1。 卡方检验的一般步骤： “皮尔森卡方检验”可用于两种情境的变项比较：适配度检验，和独立性检验。 “适配度检验”验证一组观察值的次数分配是否异于理论上的分配。 “独立性检验”验证从两个变量抽出的配对观察值组是否互相独立（例如：每次都从A国和B国各抽一个人，看他们的反应是否与国籍无关）。 不管哪个检验都包含三个步骤： 计算卡方检验的统计值“ $\chi ^{2}$ ”：把每一个观察值和理论值的差做平方后、除以理论值、再加总。 计算 $\chi ^{2}$ 统计值的自由度“ $df$”。 依据研究者设定的置信水准，查出自由度为 $df$ 的卡方分配临界值，比较它与第1步骤得出的 $\chi ^{2}$ 统计值，推论能否拒绝虚无假设。 F检验https://zh.wikipedia.org/wiki/F%E6%A3%80%E9%AA%8C 通常的F检验例子包括： 假设一系列服从正态分布的母体，都有相同的标准差。这是最典型的F检验，该检验在方差分析（ANOVA）中也非常重要。 假设一个回归模型很好地符合其数据集要求。 F检验对于数据的正态性非常敏感，因此在检验方差齐性的时候，Levene检验, Bartlett检验或者Brown–Forsythe检验的稳健性都要优于F检验。 F检验还可以用于三组或者多组之间的均值比较，但是如果被检验的数据无法满足均是正态分布的条件时，该数据的稳健型会大打折扣，特别是当显著性水平比较低时。但是，如果数据符合正态分布，而且alpha值至少为0.05，该检验的稳健型还是相当可靠的。 F检验又叫方差齐性检验，用于检验两组服从正态分布的样本是否具有相同的总体方差，即方差齐性。 F检验步骤​ 假设两组服从不同正态分布的数据样本 (1) $\begin{eqnarray} \pmb{x}:&amp;1,\quad 2,\quad 3,\quad 1,\quad 2 \nonumber \ \pmb{y}:&amp;1,\quad 3,\quad 2,\quad 2 \nonumber \end{eqnarray}$ 我们使用F检验检查这两组数据的总体方差是否相等。F 检验的主要步骤如下: 步骤1. 设$(\pmb{x} \sim N(u_1,\sigma^2_1))以及(\pmb{y} \sim N(u_2,\sigma^2_2))$。建立零假设$(\pmb{H}_0)$和备选假设$(\pmb{H}_1)$。 步骤2. 我们选择一个合适统计量$(s = \frac{s_x^2}{s_y^2})$，其中$(s_x^2)$为$(\pmb{x})$的样本方差，$(s_y^2)$为$(\pmb{y})$的样本方差。 步骤3. 查F检验临界值表。样本$(\pmb{x})$有5个数据，因此我们采用分子自由度为4所对应的列；样本$(\pmb{y})$有4个数据，因此我们采用分母自由度为3所对应的大行；显著性水平为$(\alpha=0.10)$，我们采用$(p=0.10)$所对应的小行。 查表所得值为28.71。(s = 0.972 &lt; 28.71)，故我们接受零假设，认为$(\pmb{H}_0: \sigma_1^2 = \sigma_2^2 )$。 总结1，t检验 ​ 单总体方差未知，检验单总体均值是否= or &gt;= or &lt;= $\mu_0$; ​ 两总体方差未知，检验两总体均值是否= or &gt;= or &lt;=，是否相互独立导致统计量不同; 2，卡方检验 ​ 检验单总体方差是否= or &gt;= or &lt;= $\sigma_0$; 3，F检验 ​ 检验两相互独立总体方差是否= or &gt;= or &lt;=； 相应的统计量都很容易找到，注意适用条件!!]]></content>
      <tags>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何读懂腰椎MRI核磁片]]></title>
    <url>%2F2017%2F171006_%E5%A6%82%E4%BD%95%E8%AF%BB%E6%87%82%E8%85%B0%E6%A4%8EMRI%E6%A0%B8%E7%A3%81%E7%89%87%2F</url>
    <content type="text"><![CDATA[如何读懂腰椎MRI核磁片作者：Dr. Douglas M. Gillard DC 翻译：Daniel 本文是由本博客作者翻译自Dr,Douglas，有条件的读者建议直接去看英文解释可能会更有收获。因为很多程序员都有腰椎的问题，我自己也是有一些小问题，在拍了MRI后，除了被医生敷衍几句，可能得不到更多有效的信息。这篇文章的英文原文是国内很多如何读懂MRI相关教程的来源，但我觉得他们翻译的并不是很好，有些地方会让新手产生困惑，因此我自己再翻译了一遍，希望可以帮助到一些人。 注意：在阅读本文之前，你需要了解到MRI的结果可能只能解释疼痛原因的一小部分，并且在没有体检结果，病史，多年经验的医生的指导下，MRI结果是完全无用的。所以，千万不要尝试用在这里学到的信息去揣测你的医生。请记住：在涉及椎间盘突出病例中，MRI具有至少30%的假阳性率。所以，仅仅因为你有一个椎间盘突出了，并不意味着这是你疼痛的原因。 1.如何定位椎间盘：定位像定位像非常重要，它告诉了我们哪一个椎间盘轴向图像（椎间盘俯视图）对应哪一个椎间盘。 例如图1.1的定位像中，编号10的直线告诉我们这个位置的切片是10号轴向视图。这个切片的位置正好位于L4/L5椎间盘之间的底部。 MRI医生有专业的查看软件，可以在定位像上任意移动鼠标，同时显示对应位置的轴向和矢状面（侧面）视图。 图1.1 2.轴向视图图2.1是一张T1加权成像的轴向L5/S1椎间盘MRI图像。左图和右图的区别仅在于右图有一些额外的彩色标记，来帮助我们了解各个结构，在学习之后，你可以通过观察左侧图像来练习。 图2.1我们的椎间盘是橄榄球形的结构（黄色轮廓线），橙色描绘的像弹弓的一样的结构是我们的后弓。后弓由棘突（spinous process， 图中SP)，椎板(lamina, 图中LAM)和小关节(facet)组成，而小关节本身由两部分组成：骶骨的上关节突起（SAP，因为这是L5/S1椎间盘，S1就是骶骨)和L5的下关节突起。在后弓这个弹弓结构的怀抱中，就是椎管，内部包裹着神经组织，看起来有点像米老鼠的头。米老鼠的头部（红色轮廓线）是硬膜囊，保护着内部的下垂的神经根，硬膜囊内充满了脑脊液；米老鼠的两个耳朵（绿色轮廓线）是贯穿神经根（trasversing nerve roots, 译者注：我理解为贯穿神经根会待继续在椎管内，暂时还不会伸出脊柱，对应的概念是exiting nerve roots, 发出神经根，就是从该节神经孔中出来的神经根，具体可以参看图2.3中神经根从椎管离开 ）跳进骶骨骶孔，然后从下一节骶骨椎间孔退出。紫红色轮廓线圈住的是可见的L5发出神经根，因为他们是发出神经根，你只能看到背部神经节，这些神经节通过叫做椎间孔的结果离开脊柱（见图2.3）。 绿色圆圈所示的贯穿神经根是在侧隐窝区域（图中未标注），这里是椎间盘突出最常见部位。 T1加权像不能显示组织的水分，因此，我们无法看到硬膜囊内的脑脊液，和椎间盘的髓核含水情况。 图2.2这张图展示了一名45岁男性健康L4/L5椎间盘的T2加权轴像。由于T2加权像能够显示含水量，我们可以明显看到椎间盘的中心的髓核（浅色中心），外围是颜色较暗的纤维环。我们也可以清楚地看到硬膜囊内的神经根，注意他们是怎么排列的。其他结构都如图2.1所示。这是一个非常健康的椎间盘，很多时候，由于失去水分，你可能会看不到浅色的髓核。 图2.3 3. 侧面视图图3.1是腰椎T2加权像的侧面视图，或称为矢状面视图。 首先看一些基本结构，椎间盘，位于两个椎骨之间，内部应为白色（含水），在这张图中，我们看到L5/S1椎间盘有黑化迹象（脱水），这代表有轻度的椎间盘退变性疾病。 图中蓝色小箭头指向的是后纵韧带，在影像中显示为一条在椎体和椎间盘后方向下运行的黑色线条。神奇的是，尽管这张图的患者有9毫米的突出，后纵韧带仍然包裹住了突出物，且没有破损，但显然后纵韧带受到突出物挤压，一部分从椎骨表面脱离，这种情况被称为包含性椎间盘脱出。 硬膜囊在图中是一种“超白色”结构，位于中央椎管。内部包含了自由浮动的脊神经根（马尾），包括运动和感觉神经纤维。 绿色星星指向的是黄韧带，它位于每个椎骨之间，增加脊柱的稳定性。这种结构增大/增厚会引起可怕的椎管狭窄，常见于老年患者。 图3.2是T2加权像侧面视图，与图3.1不同的是，它的侧面切面位置在边缘，因此它能展示出非常重要的神经孔和位于其中的发出神经根。这是非常重要的切片，医生应该在两个侧边都要进行仔细查看，以确保这里没有发生椎间盘突出。通常来说，椎间盘突出不会在这个区域发生，但是如果是这种情况，会引起非常严重的坐骨神经痛，手术也有较高难度。 图3.2 4. 例子图4.1是一个真实病例，左侧是L5/S1的轴向视图，右侧是L5/S1的侧面视图，并且这是T1加权像。 图4.1红色星星所在位置是一个9毫米大的突出物。注意，这个突出物已经将患者右侧的S1贯穿神经根（图中左侧）完全遮住了，并将其压迫到椎板上（绿色小箭头）。同时硬膜囊被这个突出物也严重挤压，这在轴向和侧面视图中都能看得很清楚。 这是个24岁的年轻人，他最终选择保守治疗，没有进行手术，最终恢复的不错，但是他注定是和繁重体力劳动无缘了。]]></content>
      <tags>
        <tag>Health</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习ReLearn Part1]]></title>
    <url>%2F2017%2F170628_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%80%2F</url>
    <content type="text"><![CDATA[机器学习-第一章 绪论 1.4 归纳偏好 机器学习算法在学习过程中对某种类型假设的偏好，称为“归纳偏好”（inductive bias)。归纳偏好可以看做学习算法在一个可能很庞大的假设空间中对假设进行选择的启发式或者“价值观”。有没有一般性原则来引导算法确立“正确”的偏好呢？那就是“奥卡姆剃刀”原则：若有多个假设与观察一致，选择最简单的那个。 No free lunch 理论 无论一种学习算法$$L_a$$多么聪明，学习算法$$L_b$$多么笨拙，它们在整个假设空间中的期望性能是相同的。 就像这张图展示的，A曲线应该更合理，更符合奥卡姆剃刀原则，但你不能否认另一种归纳偏好与B曲线更好符合的存在。这样来看，聪明的算法与随机乱猜算法，如果考虑所有的归纳偏好的可能性，他们的性能是一样的。 但NFL定理又一个重要前提：所有问题出现的机会相同、或所有问题同等重要。但实际情形并非这样，大部分时候，我们只关注自己正在试图解决的问题。所以NFL最重要的寓意是让我们清楚认识到，脱离具体问题，空泛地谈论“什么学习算法更好”毫无意义，因为若考虑所有潜在的问题，则所有学习算法都一样好。要谈论算法的优劣，必须要针对具体的学习问题。]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux文本编辑器介绍与使用]]></title>
    <url>%2F2017%2F180401_%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91%2F</url>
    <content type="text"><![CDATA[VimVim的命名： Vi +IMproved Vim的使用可以大致分为两种模式：命令行模式（command mode)和插入模式(insert mode)。 i切换进入插入模式，类似的，也可以使用a,o进入插入模式，只是插入的点位有区别。 在命令行模式下： 删除文字： x每按一次，删除光标所在位置后面一个字符 6x,删除光标所在位置后面6个字符 X,删除光标所在位置前面一个字符 dd，删除光标所在行 6dd，从光标所在行开始删除6行 复制 复制和y有关，并且都要搭配p命令才能完成粘贴 yw 将光标所在之处到字尾的字符复制到缓冲区 6yw复制6个字符到缓冲区 yy复制光标所在行到缓冲区 6yy复制光标下面6行文字到缓冲区 p将缓冲区内的字符贴到光标所在位置 撤销 u 撤销上一个操作，可以连续撤销 跳至制定行 ctrl+g 列出光标所在行号 15G 移动光标至第15行行首 last line mode 进入last line mode,先确保处在命令行模式，然后按:即可进入` :set nu 列出行号 :15 跳至15行 :/keyword 查找keyword，可以一直按n跳至下一个匹配 :w 保存文件 :qw 保存并退出vim SEDsed（stream editor)是一个批处理（非交互式）编辑器，它可以变换来自文件或标准输入的输入流，常用作管道中的过滤器。由于sed仅仅对输入扫描一次，因此它比其他的交互式编辑器更加高效。 sed命令行语法： sed [-n] program [file-list] 比如说，将lines这个文件中出现line的行都打印出来： cat lines Line one. The second line. Third. Line four. sed -n &#39;/line/ p&#39; lines The second line. Line four. 上述命令中，/line/是字符串的正则表达式， p显示选定的行, -n表示仅仅显示选定的行，如果不加-n，所有行都会被输出到标准输出，并且被选中的行会输出两遍。 sed program a(append) sed &#39;2a after&#39; lines 在第二行之后增加’after’ i(insert) sed &#39;2i before&#39; lines 在第二行之前增加’before’ p(print) r(read) file sed &#39;2r file_name&#39; lines 在第二行之后挂上file_name里的内容。 AWKawk是一种模式扫描和处理语言，由它的三个作者姓名的首字母命名。它搜索一个或者多个文件，已查看这些文件中是否存在匹配指定模式的记录（通常是文本行）。每次发现匹配的记录时，它通过执行动作的方式（比如将该记录写到标准输出或者将某个计数器递增）来处理文本行。与过程语言相反，AWK属于数据驱动语言：用户描述想要处理的数据并告诉AWK当他发现这些数据时如何处理它们。 AWK用法和SED很像，直接看例子吧。 cat cars plym fury 1970 73 2500 chevy malibu 1999 60 3000 ford mustang 1965 45 10000 volvo s80 1998 102 9850 chevy malibu 2000 50 3500 bmw 325i 1985 115 450 gawk &#39;{print }&#39; cars plym fury 1970 73 2500 chevy malibu 1999 60 3000 ford mustang 1965 45 10000 … gawk &#39;/chevy/&#39; cars chevy malibu 1999 60 3000 chevy malibu 2000 50 3500 gawk &#39;/chevy/ {print $3, $1}&#39; cars 选中包含字符串’chevy’的所有行并显示选中行的第三个字段和第一个字段 1999 chevy 2000 chevy gawk &#39;$5 &lt;= 3000&#39; cars plym fury 1970 73 2500 chevy malibu 1999 60 3000 chevy malibu 2000 50 3500 bmw 325i 1985 115 450 CAT连接并显示文件，将文件复制到标准输出。可以使用cat在屏幕上显示一个或者多个文本文件内容。 cat [options] [file_list] file_list为cat要处理的一个或者多个文件的路径名列表。如果不指定任何参数，或者指定一个连字符（-）代替文件名，cat就从标准输入读取输入信息。 cat file1 file2 &gt; output_file 将file1和file2合并为output_file 在不使用编辑器的情况下，可以使用cat创建较短的文本文件。 cat &gt; new_file 从标准输入输入内容 Ctrl+D 结束输入(发出EOF信号) who | cat header - footer &gt; output 管道将who的输出发送到cat的标准输入，shell将cat的输出重定向到文件output中，output文件将包括header,who输出结果，和footer。]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop与Spark区别和联系]]></title>
    <url>%2F2017%2F170626_Hadoop%E4%B8%8ESpark%E5%8C%BA%E5%88%AB%E5%92%8C%E8%81%94%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[公司的集群用的是spark系统，但一直就是这么稀里糊涂的用着，这里只是简单梳理一下各种名称的含义和背后的关系。 Hadoop 1.0是由分布式存储系统HDFS和分布式计算框架MapReduce组成， Hadoop 2.0在此基础，针对MapReduce在扩展性和多框架支持方面的不足，提出了全新的资源管理框架YARN(yet another resource negotiator) YARN是在Hadoop MapReduce基础上演化而来的，在MapReduce时代，很多人批评MapReduce不适合迭代计算和流失计算，于是出现了Spark和Storm等计算框架，而这些系统的开发者则在自己的网站上或者论文里与MapReduce对比，鼓吹自己的系统多么先进高效，而出现了YARN之后，则形势变得明朗：MapReduce只是运行在YARN之上的一类应用程序抽象，Spark和Storm本质上也是，他们只是针对不同类型的应用开发的，没有优劣之别，各有所长，合并共处，而且，今后所有计算框架的开发，不出意外的话，也应是在YARN之上。这样，一个以YARN为底层资源管理平台，多种计算框架运行于其上的生态系统诞生了。 Spark只是一种内存计算框架，用Scala编写，它并没有自己的文件管理系统，所以Spark一般是依附于Hadoop的HDFS。可以将Spark理解为带cache的MapReduce框架。你看Spark官网的评价： Spark is a MapReduce-like cluster computing framework designed for low-latency iterative jobs and interactive use from an interpreter. Hive是Cloudera发起的项目，一开始是为了将sql语言转换为mapreduce流程所设计，但后来开发了hive on spark，就能将Spark作为Hive的计算引擎，用类SQL语言实现计算，称为H(ive)QL。但Spark本身自己也有一套SQL查询模块，叫做Spark-sql。 SparkSQL和Hive On Spark都是在Spark上实现SQL的解决方案。Spark早先有Shark项目用来实现SQL层，不过后来推翻重做了，就变成了SparkSQL。这是Spark官方Databricks的项目，Spark项目本身主推的SQL实现。Hive On Spark比SparkSQL稍晚。Hive原本是没有很好支持MapReduce之外的引擎的，而Hive On Tez项目让Hive得以支持和Spark近似的Planning结构（非MapReduce的DAG）。所以在此基础上，Cloudera主导启动了Hive On Spark。这个项目得到了IBM，Intel和MapR的支持（但是没有Databricks）。 需要理解的是，Hive和SparkSQL都不负责计算，它们只是告诉Spark，你需要这样算那样算，但是本身并不直接参与计算。 Hbase： Hadoop database 的简称，也就是基于Hadoop数据库，是一种NoSQL数据库 一个数据仓库的架构基本就是：底层HDFS，上面跑MapReduce/Tez/Spark，在上面跑Hive，Pig。 HBase is an open-source, distributed, column-oriented database built on top of HDFS (or KFS) based on BigTable!]]></content>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
</search>
