<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[public_loan_component]]></title>
    <url>%2F2019%2F08%2F12%2Fpublic-loan-component%2F</url>
    <content type="text"><![CDATA[社融是什么社会融资规模(社融)指的是实体经济从金融体系拿到的钱。可以是金融机构直接给的钱，比如贷款、保险赔偿金等；也可以是金融机构帮着拉皮条搞来的钱，比如非金融企业发行股票、债券等。 虽然是中国特有的经济指标，不过社融基本上可以等价于IMF（国际货币基金组织)倡导的“信用总量”的概念。反映的是实体经济对货币的需求量。 社融统计的是境内机构和个人投资到实体经济的钱，所以不统计境外机构的投资，也不统计金融机构之间的融资，如同业拆借。 社融的组成 表内融资：人民币贷款指的是金融机构出借给非金融企业/机关团体/个人的人民币贷款(贷款合同、票据贴现、垫款、贸易融资等形式都算)。个人房贷是属于该项目下。 外币贷款和人民币贷款一样，只不过币种是外币。 人民币贷款和外币贷款都是“表内业务”，金融机构发放贷款时需要把帐记到资产负债表里头，靠利差来赚钱。 监管会时不时地来查查表，而且还要求银行需要针对这些业务上交存款准备金，还要满足资本充足率等要求 而下面的这三个则是“表外业务”。金融机构主要做的是拉皮条的活，收手续费。资产不入表，所以也没有上面提到的限制。 表外融资（非标融资）： 委托贷款A要跟B借一笔钱，利率期限什么都谈好了，但是B没有放贷资质，所以B委托银行来当个中间人。这笔放款就叫做委托贷款。 信托贷款投资者可以把钱投资到资金信托产品里头(可以简单理解为信托公司卖的基金)，信托公司会把一部分钱拿去放贷，这部分就是信托贷款。 未贴现银行承兑汇票企业A跟B做生意，A欠B钱，于是开了张欠条(汇票)给B。并且A找来了银行，银行承诺一定期限后B可以拿这张票来换钱。B为了早点拿到钱，也可以提前把这张欠条打折卖给银行(打折出售哈银行的过程披称为“贴现”)。“未贴现银行承兑汇票”这个指标指的是B还hold着、还没有打折出售的那部分汇票。 从社融存量的结构上来看的话，人民币贷款占了接近七成。 企业债券融资，非金融企业境内股票融资都比较好理解。 地方政府专项债券，该项融资主要对接公路建设、棚改等基建项目，属于向实体经济提供资金范围]]></content>
  </entry>
  <entry>
    <title><![CDATA[快速检测时间序列是否为平稳序列]]></title>
    <url>%2F2018%2F12%2F17%2F181217_%E5%BF%AB%E9%80%9F%E6%A3%80%E6%B5%8B%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%98%AF%E5%90%A6%E4%B8%BA%E5%B9%B3%E7%A8%B3%E5%BA%8F%E5%88%97%2F</url>
    <content type="text"><![CDATA[对于没有学习过随机过程课的我而言，判定一个序列是否为平稳主要靠看图说话…而且机器学习跑模型，一般而言也不需要了解特征这块的性质，但最近做量化模型比较多，如果一个特征是非平稳的话，它可能会给模型一些不好的先验，这是我想避免的。所以这篇文章主要讲一下如何快速判断一个时间序列是否是平稳的。 需要了解一个概念，一般非平稳序列都有单位根(unit root)，所以可以使用单位根检验来判定一个序列是否是平稳，最常用的检验就是A.D.Fuller检验。我把上证综指的日收益率序列画出来，基本上可以看到还是比较平稳的。1234567891011import bottleneck as bnrolmean = bn.move_mean(timeseries, window=14)rolstd = bn.move_std(timeseries, window=14)#Plot rolling statistics:orig = plt.plot(timeseries, color='blue',label='Original')mean = plt.plot(rolmean, color='red', label='Rolling Mean')std = plt.plot(rolstd, color='black', label = 'Rolling Std')plt.legend(loc='best')plt.title('Rolling Mean &amp; Standard Deviation')plt.show(block=False) 下面对这个时间序列使用AD Fuller检验，statsmodels里有现成的函数可以调用。1234567891011121314151617from statsmodels.tsa.stattools import adfullerdftest = adfuller(timeseries, autolag='AIC')dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])for key,value in dftest[4].items(): dfoutput['Critical Value (%s)'%key] = value print (dfoutput) -------------------------------------------Results of Augment Dickey-Fuller Test:Test Statistic -1.266111e+01p-value 1.304351e-23#Lags Used 1.400000e+01Number of Observations Used 3.025000e+03Critical Value (1%) -3.432514e+00Critical Value (5%) -2.862496e+00Critical Value (10%) -2.567279e+00dtype: float64 上面把AD Fuller检验结果也打印出来了。一般我就看一下p-value是否小于0.05，或者看Test Statistic是否小于*Critical Value(5%)，如果小于的话，就表示这是平稳序列。]]></content>
      <tags>
        <tag>Machine Learning</tag>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Xgboost特征重要性的可视化]]></title>
    <url>%2F2018%2F12%2F16%2F181217_Xgboost%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[对于决策树模型而言，其相比神经网络一个优势就是还能保留一点的可解释性。当然对于GBDT树，除了第一颗树我们可以从决策树的角度来理解模型是怎么工作，后面的所有树其实都是在学习残差，所以也不是很容易直观理解了。但无论如何，我们总是可以很方便的统计所有树模型的特征的重要性，重要性的衡量依据可以是特征使用次数，也可以特征增益。 对于XGboost模型，一般使用这行代码把特征重要性打印出来1model.get_fscore() 如果是lightGBM模型，改为这行代码1model.feature_importance() 我最近在训练模型的时候，希望能够看到模型每一步/每几步迭代过程中的特征使用情况，而不只是在模型全部训练完之后看一个整体。所以就写了以下代码，能够把模型迭代过程中的特征使用情况以及变化趋势可视化地展示出来。 模型训练我们用sklearn提供的wine dataset作为我们的toy example.12345678from sklearn import datasetsimport pandas as pdimport numpy as npimport xgboost as xgbwine = datasets.load_wine()X = wine['data']y = wine['target'] 训练集，测试集分割，构造DMatrix12345from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)dtrain = xgb.DMatrix(data=X_train, label=y_train, feature_names=wine['feature_names'])dtest = xgb.DMatrix(data=X_test, label=y_test, feature_names=wine['feature_names']) 定义XGboost训练参数1234default_param = &#123;'booster':'gbtree', 'silent':0, 'objective':'multi:softmax', 'num_class':3, 'eta':0.1, 'max_depth':4, 'eval_metric':'mlogloss', 'tree_method':'hist'&#125;num_round = 100 因为我们需要记录模型迭代过程中特征重要性，所以需要写一个callback函数让xgboost在每一轮迭代后调用，这个函数的作用就是记录下当时模型的特征重要性。123456789def record_fscore(fscore_result): if not isinstance(fscore_result, dict): raise TypeError('fscore_result has to be a dictionary') fscore_result.clear() def call_back(env): if env.iteration % 2 == 0: #每两轮记录一次 fscore_result[env.iteration] = env.model.get_fscore() return call_back 模型训练12345fscore_result = &#123;&#125;model = xgb.train(default_param, dtrain, num_boost_round=num_round, evals=[(dtrain, 'train'), (dtest, 'test')], verbose_eval = True, callbacks = [record_fscore(fscore_result)]) 训练完成后，fscore_result是这样子的：123456789101112print (fscore_result)&#123;0: &#123;'proline': 2, 'ash': 1, 'flavanoids': 2, 'alcalinity_of_ash': 1, 'color_intensity': 2, 'alcohol': 1&#125;, 2: &#123;'proline': 6, 'ash': 1, 'flavanoids': 6,... 特征可视化特征可视化我们使用plotly,相比matplotlib的优点在于它会生成一个可交互的html页面，并且它的heatmap灵活性比matplotlib强太多了。 首先定义一个后面会用到的辅助函数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def array_map(arr, mapping, default=np.NaN, on_missing='raise', return_pandas=False): """ 把arr的值，通过给定的一个映射关系mapping，应设成mapping中的value mapping: dict/pd.Series/(key_array, value_array) default: 如果有值不存在于mapping中，fill成default on_missing: 'raise'/'fill', 默认如果有不存在的值，会抛出异常，否则用default填充 return_pandas: 如果mapping是Series或者DataFrame，是否返回Series和DataFrame（默认为False，返回ndarray） example: &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; import pandas as pd &gt;&gt;&gt; arr = np.array([1, 2, 1, 3, 2]) &gt;&gt;&gt; mapping = &#123;0:1, 1:0, 2:3, 3:2&#125; &gt;&gt;&gt; array_map(arr, mapping) array([0, 3, 0, 2, 3]) &gt;&gt;&gt; array_map(arr, pd.Series([0, 1, 2, 3], index=[1, 0, 3, 2])) array([0, 3, 0, 2, 3]) """ if isinstance(mapping, tuple): arr_key = np.array(mapping[0]) arr_value = np.array(mapping[1]) if len(arr_value.shape) == 0: arr_key, arr_value = np.broadcast_arrays(arr_key, arr_value) if on_missing == "raise": idx_mapping = pd.Series(np.arange(arr_key.shape[0], dtype=np.uint32), index=arr_key) else: arr_value = np.append(arr_value, np.expand_dims(default, axis=0), axis=0) idx_mapping = pd.Series(np.arange(arr_key.shape[0], dtype=np.uint32), index=arr_key) mapped_indices = array_map(arr, idx_mapping, default=arr_key.shape[0], on_missing=on_missing, return_pandas=False) return arr_value[mapped_indices] ks, indices = np.unique(arr, return_inverse=True) if isinstance(mapping, pd.Series) or isinstance(mapping, pd.DataFrame): if on_missing == 'raise': n_idx = np.unique(mapping.index.values) n_all = np.unique(np.concatenate([ks, mapping.index.values])) if n_all.shape[0] &gt; n_idx.shape[0]: raise Exception(f"MISSING VALUE IN MAPPING: &#123;set(n_all) - set(n_idx)&#125;") ans = mapping.reindex(ks, fill_value=default).values[indices] if return_pandas: if isinstance(mapping, pd.Series): return pd.Series(ans, index=ks[indices]) elif isinstance(mapping, pd.DataFrame): return pd.DataFrame(ans, columns=mapping.columns, index=ks[indices]) else: return ans else: if on_missing == 'raise': return np.array([mapping[k] for k in ks])[indices] else: return np.array([mapping.get(k, default) for k in ks])[indices] 开始画图了12345678910111213141516from plotly.offline import plotimport plotly.graph_objs as goif not isinstance(fscore_result, dict): raise TypeError(&apos;fscore_result should be a dictionary&apos;)feature_list = dtrain.feature_namesrounds = list(fscore_result.keys())num_round = len(fscore_result)num_feat = len(feature_list)z = np.zeros((num_round, num_feat), dtype=np.int)for i, round in enumerate(rounds): z[i,:] = array_map(feature_list, fscore_result[round], on_missing=False, default=0.0001)z_prev = np.pad(z, ((1,0),(0,0)), mode=&apos;constant&apos;, constant_values=0)[:-1]z_inc = z - z_prev z_inc 即为模型每轮新增使用的特征计数，下面就是画heatmap的代码。123456789101112data= [go.Heatmap( ​ z=z_inc,​ x=feature_list,​ y=np.arange(num_round),​ colorscale=&apos;Viridis&apos;,​ )]layout = go.Layout( title=&apos;Feature Importance&apos;)fig = go.Figure(data=data, layout=layout)plot(fig, filename=&quot;feat_importance_heatmap.html&quot;) 最终生成的html如图 相比只看一个最终整体的特征重要性，观察每轮/每几轮 模型迭代使用的特征情况还是能够带来一些新信息的，我们可以看到有些特征，一开始有些用，到后期就没有用了，而有些特征则一直都很用。]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对比T检验卡方检验和F检验]]></title>
    <url>%2F2018%2F04%2F01%2F180401_T%E6%A3%80%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[T检验，卡方检验和F检验是最常见的三大统计假设检验方法，本文将主要介绍在何种情况下，如何使用某一种检验方法，但对具体的原理不做过多解释，重在实用。 T检验T检验，针对的是样本均值。假设总体均值$\mu=1.1$已知，现有一组服从正态分布的数据样本集合： 1(1) 1.0, 1.2, 1.4, 1.1, 1.3, 1.2 能否判断这组数据样本是否来自于已知总体，限定显著性水平$\alpha=0.05$. 解： 步骤一：我们选择T统计量。T统计量的计算公式如下 $$T= \frac{\hat{x}-1.1}{s/ \sqrt{n}}$$ 其中，$\hat{x}$是样本均值，$s$为样本标准差，计算公式为$s=\sqrt{\frac{1}{n-1} \sum{i=1}^{n}(x{i}-\hat{x})^2}$ 计算得到 $T=0.28867$ 步骤二：查T检验临界值表。因为样本中拥有6份数据，因此采用$n=5$（自由度为5）所对应的行；显著性水平$\alpha=0.05$，因此采用双侧检验$p=0.05$所对应的列。 可以看到，查表值为2.571， 由于$T=0.28867&lt;2.571，所以我们可以认为样本是来自于$\mu=1.1$的总体。 t检验类型 ​ t检验有多种类型，可以分为只有一组样本的单体检验和有两组样本的双体检验。单体检验用于检验样本的分布期望是否等于某个值。双体检验用于检验两组样本的分布期望是否相等，又分为配对双体检验和非配对双体检验。配对双体检验的两组样本数据是一一对应的，而非配对双体检验的两组数据则是独立的。比如药物实验中，配对双体检验适用于观察同一组人服用药物之前和之后，非配对双体检验适用于一组服用药物而一组不服用药物。 ​ 1）单体检验​ 单体检验是针对一组样本的假设检验。零假设为$\pmb{H}_0:u=u_0$,统计量 $t = \frac{\bar{x} - \mu_0}{s/\sqrt{n}}$服从自由度$n-1$的 T 分布。 ​ 2）配对双体检验​ 配对双体检验针对配对的两组样本。配对双体检验假设两组样本之间的差值服从正态分布。如果该正态分布的期望为零，则说明这两组样本不存在显著差异。零假设为 $\pmb{H}_0:u=u_0$。统计量$t = \frac{\bar{d} - \mu_0}{s/\sqrt{n}}$服从自由度为 $n-1$ 的 T 分布，其中 $\bar{d}$ 是差值的样本均值，$s$是差值的样本标准差。 ​ 3）非配对双体检验​ 非配对双体检验针对独立的两组样本。非配对双体检验假设两组样本是从不同的正态分布采样出来的。根据两个正态分布的标准差是否相等，非配对双体检验又可以分两类。一种是分布标准差相等的情况。零假设是两组样本的分布期望相等，统计量 T 服从自由度为 $n_1+n_2-2$l的 T 分布。 $$\begin{eqnarray} t &amp;=&amp; \frac{\bar{x_1}-\bar{x2}}{s{x_1,x_2} \sqrt{1/n_1+1/n2}} \nonumber\ s{x_1,x_2} &amp;=&amp; \sqrt{ \frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2} } \nonumber \end{eqnarray}$$ 其中 $\bar{x_1}]$和 $\bar{x_2}$分别是两组样本的样本均值，$n_1$ 和$n_2$分别为两组样本的大小，$s_1$ 和 $s_2$分别是两组样本的样本标准差。另一种是分布标准差不相等的情况。零假设也是两组样本的分布期望相等，统计量 T 服从 T 分布。 $\begin{eqnarray} t = \frac{\bar{x}_1-\bar{x}_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}} \nonumber \end{eqnarray}$ T 分布的自由度为 $\begin{eqnarray} d.f. = \frac{ (s_1^2/n_1+s_2^2/n_2)^2 }{(s_1^2/n_1)^2/(n_1-1)+ (s_2^2/n_2)^2/(n_2-1)} \nonumber \end{eqnarray}$ Z检验（T检验的变体）Z检验并不能算是一种全新的统计检验方法，刚刚的T检验，我们是只知道总体的均值$\mu$,但不知道总体的方差。如果我们既知道总体的均值，也知道总体的方差$\sigma$，那此时就适用Z检验。 此时Z-score统计量公式为： $$Z= \frac{\hat{x}-1.1}{\sigma/ \sqrt{n}}$$ Z检验就是查正态分布表。因为Z-score是完全服从$N(0,1)$正态分布，事实上，当n的数量足够大时（一般&gt;45)，T检验都可以不用查表了，直接查正态分布表即可。 $\chi$检验又称Chi-Squared Test，最有名的是皮尔逊卡方检验，维基百科对此有很好的描述。 皮尔森卡方检验（英语：Pearson’s chi-squared test）是最有名卡方检验之一（其他常用的卡方检验还有叶氏连续性校正、似然比检验、一元混成检验等等－－它们的统计值之机率分配都近似于卡方分配，故称卡方检验） “皮尔森卡方检验”的虚无假设（H0）是：一个样本中已发生事件的次数分配会遵守某个特定的理论分配。 在虚无假设的句子中，“事件”必须互斥，并且所有事件总机率等于1。或者说，每个事件是类别变量（英语：categorical variable）的一种类别或级别（英语：level）。 简单的例子：常见的六面骰子，事件＝丢骰子的结果（可能是1~6任一个）属于类别变量，每一面都是此变量的一种（一个级别）结果，每种结果互斥（1不是2, 3, 4, 5, 6; 2不是1, 3, 4 …），六面的机率总和等于1。 卡方检验的一般步骤： “皮尔森卡方检验”可用于两种情境的变项比较：适配度检验，和独立性检验。 “适配度检验”验证一组观察值的次数分配是否异于理论上的分配。 “独立性检验”验证从两个变量抽出的配对观察值组是否互相独立（例如：每次都从A国和B国各抽一个人，看他们的反应是否与国籍无关）。 不管哪个检验都包含三个步骤： 计算卡方检验的统计值“ $\chi ^{2}$ ”：把每一个观察值和理论值的差做平方后、除以理论值、再加总。 计算 $\chi ^{2}$ 统计值的自由度“ $df$”。 依据研究者设定的置信水准，查出自由度为 $df$ 的卡方分配临界值，比较它与第1步骤得出的 $\chi ^{2}$ 统计值，推论能否拒绝虚无假设。 F检验https://zh.wikipedia.org/wiki/F%E6%A3%80%E9%AA%8C 通常的F检验例子包括： 假设一系列服从正态分布的母体，都有相同的标准差。这是最典型的F检验，该检验在方差分析（ANOVA）中也非常重要。 假设一个回归模型很好地符合其数据集要求。 F检验对于数据的正态性非常敏感，因此在检验方差齐性的时候，Levene检验, Bartlett检验或者Brown–Forsythe检验的稳健性都要优于F检验。 F检验还可以用于三组或者多组之间的均值比较，但是如果被检验的数据无法满足均是正态分布的条件时，该数据的稳健型会大打折扣，特别是当显著性水平比较低时。但是，如果数据符合正态分布，而且alpha值至少为0.05，该检验的稳健型还是相当可靠的。 F检验又叫方差齐性检验，用于检验两组服从正态分布的样本是否具有相同的总体方差，即方差齐性。 F检验步骤​ 假设两组服从不同正态分布的数据样本 (1) $\begin{eqnarray} \pmb{x}:&amp;1,\quad 2,\quad 3,\quad 1,\quad 2 \nonumber \ \pmb{y}:&amp;1,\quad 3,\quad 2,\quad 2 \nonumber \end{eqnarray}$ 我们使用F检验检查这两组数据的总体方差是否相等。F 检验的主要步骤如下: 步骤1. 设$(\pmb{x} \sim N(u_1,\sigma^2_1))以及(\pmb{y} \sim N(u_2,\sigma^2_2))$。建立零假设$(\pmb{H}_0)$和备选假设$(\pmb{H}_1)$。 步骤2. 我们选择一个合适统计量$(s = \frac{s_x^2}{s_y^2})$，其中$(s_x^2)$为$(\pmb{x})$的样本方差，$(s_y^2)$为$(\pmb{y})$的样本方差。 步骤3. 查F检验临界值表。样本$(\pmb{x})$有5个数据，因此我们采用分子自由度为4所对应的列；样本$(\pmb{y})$有4个数据，因此我们采用分母自由度为3所对应的大行；显著性水平为$(\alpha=0.10)$，我们采用$(p=0.10)$所对应的小行。 查表所得值为28.71。(s = 0.972 &lt; 28.71)，故我们接受零假设，认为$(\pmb{H}_0: \sigma_1^2 = \sigma_2^2 )$。 总结1，t检验 ​ 单总体方差未知，检验单总体均值是否= or &gt;= or &lt;= $\mu_0$; ​ 两总体方差未知，检验两总体均值是否= or &gt;= or &lt;=，是否相互独立导致统计量不同; 2，卡方检验 ​ 检验单总体方差是否= or &gt;= or &lt;= $\sigma_0$; 3，F检验 ​ 检验两相互独立总体方差是否= or &gt;= or &lt;=； 相应的统计量都很容易找到，注意适用条件!!]]></content>
      <tags>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何读懂腰椎MRI核磁片]]></title>
    <url>%2F2017%2F10%2F07%2F171006_%E5%A6%82%E4%BD%95%E8%AF%BB%E6%87%82%E8%85%B0%E6%A4%8EMRI%E6%A0%B8%E7%A3%81%E7%89%87%2F</url>
    <content type="text"><![CDATA[如何读懂腰椎MRI核磁片作者：Dr. Douglas M. Gillard DC 翻译：Daniel 本文是由本博客作者翻译自Dr,Douglas，有条件的读者建议直接去看英文解释可能会更有收获。因为很多程序员都有腰椎的问题，我自己也是有一些小问题，在拍了MRI后，除了被医生敷衍几句，可能得不到更多有效的信息。这篇文章的英文原文是国内很多如何读懂MRI相关教程的来源，但我觉得他们翻译的并不是很好，有些地方会让新手产生困惑，因此我自己再翻译了一遍，希望可以帮助到一些人。 注意：在阅读本文之前，你需要了解到MRI的结果可能只能解释疼痛原因的一小部分，并且在没有体检结果，病史，多年经验的医生的指导下，MRI结果是完全无用的。所以，千万不要尝试用在这里学到的信息去揣测你的医生。请记住：在涉及椎间盘突出病例中，MRI具有至少30%的假阳性率。所以，仅仅因为你有一个椎间盘突出了，并不意味着这是你疼痛的原因。 1.如何定位椎间盘：定位像定位像非常重要，它告诉了我们哪一个椎间盘轴向图像（椎间盘俯视图）对应哪一个椎间盘。 例如图1.1的定位像中，编号10的直线告诉我们这个位置的切片是10号轴向视图。这个切片的位置正好位于L4/L5椎间盘之间的底部。 MRI医生有专业的查看软件，可以在定位像上任意移动鼠标，同时显示对应位置的轴向和矢状面（侧面）视图。 图1.1 2.轴向视图图2.1是一张T1加权成像的轴向L5/S1椎间盘MRI图像。左图和右图的区别仅在于右图有一些额外的彩色标记，来帮助我们了解各个结构，在学习之后，你可以通过观察左侧图像来练习。 图2.1 我们的椎间盘是橄榄球形的结构（黄色轮廓线），橙色描绘的像弹弓的一样的结构是我们的后弓。后弓由棘突（spinous process， 图中SP)，椎板(lamina, 图中LAM)和小关节(facet)组成，而小关节本身由两部分组成：骶骨的上关节突起（SAP，因为这是L5/S1椎间盘，S1就是骶骨)和L5的下关节突起。在后弓这个弹弓结构的怀抱中，就是椎管，内部包裹着神经组织，看起来有点像米老鼠的头。米老鼠的头部（红色轮廓线）是硬膜囊，保护着内部的下垂的神经根，硬膜囊内充满了脑脊液；米老鼠的两个耳朵（绿色轮廓线）是贯穿神经根（trasversing nerve roots, 译者注：我理解为贯穿神经根会待继续在椎管内，暂时还不会伸出脊柱，对应的概念是exiting nerve roots, 发出神经根，就是从该节神经孔中出来的神经根，具体可以参看图2.3中神经根从椎管离开 ）跳进骶骨骶孔，然后从下一节骶骨椎间孔退出。紫红色轮廓线圈住的是可见的L5发出神经根，因为他们是发出神经根，你只能看到背部神经节，这些神经节通过叫做椎间孔的结果离开脊柱（见图2.3）。 绿色圆圈所示的贯穿神经根是在侧隐窝区域（图中未标注），这里是椎间盘突出最常见部位。 T1加权像不能显示组织的水分，因此，我们无法看到硬膜囊内的脑脊液，和椎间盘的髓核含水情况。 图2.2 这张图展示了一名45岁男性健康L4/L5椎间盘的T2加权轴像。由于T2加权像能够显示含水量，我们可以明显看到椎间盘的中心的髓核（浅色中心），外围是颜色较暗的纤维环。我们也可以清楚地看到硬膜囊内的神经根，注意他们是怎么排列的。其他结构都如图2.1所示。这是一个非常健康的椎间盘，很多时候，由于失去水分，你可能会看不到浅色的髓核。 图2.3 3. 侧面视图图3.1是腰椎T2加权像的侧面视图，或称为矢状面视图。 首先看一些基本结构，椎间盘，位于两个椎骨之间，内部应为白色（含水），在这张图中，我们看到L5/S1椎间盘有黑化迹象（脱水），这代表有轻度的椎间盘退变性疾病。 图中蓝色小箭头指向的是后纵韧带，在影像中显示为一条在椎体和椎间盘后方向下运行的黑色线条。神奇的是，尽管这张图的患者有9毫米的突出，后纵韧带仍然包裹住了突出物，且没有破损，但显然后纵韧带受到突出物挤压，一部分从椎骨表面脱离，这种情况被称为包含性椎间盘脱出。 硬膜囊在图中是一种“超白色”结构，位于中央椎管。内部包含了自由浮动的脊神经根（马尾），包括运动和感觉神经纤维。 绿色星星指向的是黄韧带，它位于每个椎骨之间，增加脊柱的稳定性。这种结构增大/增厚会引起可怕的椎管狭窄，常见于老年患者。 图3.2是T2加权像侧面视图，与图3.1不同的是，它的侧面切面位置在边缘，因此它能展示出非常重要的神经孔和位于其中的发出神经根。这是非常重要的切片，医生应该在两个侧边都要进行仔细查看，以确保这里没有发生椎间盘突出。通常来说，椎间盘突出不会在这个区域发生，但是如果是这种情况，会引起非常严重的坐骨神经痛，手术也有较高难度。 图3.2 4. 例子图4.1是一个真实病例，左侧是L5/S1的轴向视图，右侧是L5/S1的侧面视图，并且这是T1加权像。 图4.1 红色星星所在位置是一个9毫米大的突出物。注意，这个突出物已经将患者右侧的S1贯穿神经根（图中左侧）完全遮住了，并将其压迫到椎板上（绿色小箭头）。同时硬膜囊被这个突出物也严重挤压，这在轴向和侧面视图中都能看得很清楚。 这是个24岁的年轻人，他最终选择保守治疗，没有进行手术，最终恢复的不错，但是他注定是和繁重体力劳动无缘了。]]></content>
      <tags>
        <tag>Health</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习ReLearn Part1]]></title>
    <url>%2F2017%2F06%2F28%2F170628_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%80%2F</url>
    <content type="text"><![CDATA[机器学习-第一章 绪论 1.4 归纳偏好 机器学习算法在学习过程中对某种类型假设的偏好，称为“归纳偏好”（inductive bias)。归纳偏好可以看做学习算法在一个可能很庞大的假设空间中对假设进行选择的启发式或者“价值观”。有没有一般性原则来引导算法确立“正确”的偏好呢？那就是“奥卡姆剃刀”原则：若有多个假设与观察一致，选择最简单的那个。 No free lunch 理论 无论一种学习算法$$L_a$$多么聪明，学习算法$$L_b$$多么笨拙，它们在整个假设空间中的期望性能是相同的。 就像这张图展示的，A曲线应该更合理，更符合奥卡姆剃刀原则，但你不能否认另一种归纳偏好与B曲线更好符合的存在。这样来看，聪明的算法与随机乱猜算法，如果考虑所有的归纳偏好的可能性，他们的性能是一样的。 但NFL定理又一个重要前提：所有问题出现的机会相同、或所有问题同等重要。但实际情形并非这样，大部分时候，我们只关注自己正在试图解决的问题。所以NFL最重要的寓意是让我们清楚认识到，脱离具体问题，空泛地谈论“什么学习算法更好”毫无意义，因为若考虑所有潜在的问题，则所有学习算法都一样好。要谈论算法的优劣，必须要针对具体的学习问题。]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux文本编辑器介绍与使用]]></title>
    <url>%2F2017%2F06%2F27%2F180401_%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91%2F</url>
    <content type="text"><![CDATA[VimVim的命名： Vi +IMproved Vim的使用可以大致分为两种模式：命令行模式（command mode)和插入模式(insert mode)。 i切换进入插入模式，类似的，也可以使用a,o进入插入模式，只是插入的点位有区别。 在命令行模式下： 删除文字： x每按一次，删除光标所在位置后面一个字符 6x,删除光标所在位置后面6个字符 X,删除光标所在位置前面一个字符 dd，删除光标所在行 6dd，从光标所在行开始删除6行 复制 复制和y有关，并且都要搭配p命令才能完成粘贴 yw 将光标所在之处到字尾的字符复制到缓冲区 6yw复制6个字符到缓冲区 yy复制光标所在行到缓冲区 6yy复制光标下面6行文字到缓冲区 p将缓冲区内的字符贴到光标所在位置 撤销 u 撤销上一个操作，可以连续撤销 跳至制定行 ctrl+g 列出光标所在行号 15G 移动光标至第15行行首 last line mode 进入last line mode,先确保处在命令行模式，然后按:即可进入` :set nu 列出行号 :15 跳至15行 :/keyword 查找keyword，可以一直按n跳至下一个匹配 :w 保存文件 :qw 保存并退出vim SEDsed（stream editor)是一个批处理（非交互式）编辑器，它可以变换来自文件或标准输入的输入流，常用作管道中的过滤器。由于sed仅仅对输入扫描一次，因此它比其他的交互式编辑器更加高效。 sed命令行语法： sed [-n] program [file-list] 比如说，将lines这个文件中出现line的行都打印出来： cat lines Line one. The second line. Third. Line four. sed -n &#39;/line/ p&#39; lines The second line. Line four. 上述命令中，/line/是字符串的正则表达式， p显示选定的行, -n表示仅仅显示选定的行，如果不加-n，所有行都会被输出到标准输出，并且被选中的行会输出两遍。 sed program a(append) sed &#39;2a after&#39; lines 在第二行之后增加’after’ i(insert) sed &#39;2i before&#39; lines 在第二行之前增加’before’ p(print) r(read) file sed &#39;2r file_name&#39; lines 在第二行之后挂上file_name里的内容。 AWKawk是一种模式扫描和处理语言，由它的三个作者姓名的首字母命名。它搜索一个或者多个文件，已查看这些文件中是否存在匹配指定模式的记录（通常是文本行）。每次发现匹配的记录时，它通过执行动作的方式（比如将该记录写到标准输出或者将某个计数器递增）来处理文本行。与过程语言相反，AWK属于数据驱动语言：用户描述想要处理的数据并告诉AWK当他发现这些数据时如何处理它们。 AWK用法和SED很像，直接看例子吧。 cat cars plym fury 1970 73 2500 chevy malibu 1999 60 3000 ford mustang 1965 45 10000 volvo s80 1998 102 9850 chevy malibu 2000 50 3500 bmw 325i 1985 115 450 gawk &#39;{print }&#39; cars plym fury 1970 73 2500 chevy malibu 1999 60 3000 ford mustang 1965 45 10000 … gawk &#39;/chevy/&#39; cars chevy malibu 1999 60 3000 chevy malibu 2000 50 3500 gawk &#39;/chevy/ {print $3, $1}&#39; cars 选中包含字符串’chevy’的所有行并显示选中行的第三个字段和第一个字段 1999 chevy 2000 chevy gawk &#39;$5 &lt;= 3000&#39; cars plym fury 1970 73 2500 chevy malibu 1999 60 3000 chevy malibu 2000 50 3500 bmw 325i 1985 115 450 CAT连接并显示文件，将文件复制到标准输出。可以使用cat在屏幕上显示一个或者多个文本文件内容。 cat [options] [file_list] file_list为cat要处理的一个或者多个文件的路径名列表。如果不指定任何参数，或者指定一个连字符（-）代替文件名，cat就从标准输入读取输入信息。 cat file1 file2 &gt; output_file 将file1和file2合并为output_file 在不使用编辑器的情况下，可以使用cat创建较短的文本文件。 cat &gt; new_file 从标准输入输入内容 Ctrl+D 结束输入(发出EOF信号) who | cat header - footer &gt; output 管道将who的输出发送到cat的标准输入，shell将cat的输出重定向到文件output中，output文件将包括header,who输出结果，和footer。]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop与Spark区别和联系]]></title>
    <url>%2F2017%2F06%2F26%2F170626_Hadoop%E4%B8%8ESpark%E5%8C%BA%E5%88%AB%E5%92%8C%E8%81%94%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[公司的集群用的是spark系统，但一直就是这么稀里糊涂的用着，这里只是简单梳理一下各种名称的含义和背后的关系。 Hadoop 1.0是由分布式存储系统HDFS和分布式计算框架MapReduce组成， Hadoop 2.0在此基础，针对MapReduce在扩展性和多框架支持方面的不足，提出了全新的资源管理框架YARN(yet another resource negotiator) YARN是在Hadoop MapReduce基础上演化而来的，在MapReduce时代，很多人批评MapReduce不适合迭代计算和流失计算，于是出现了Spark和Storm等计算框架，而这些系统的开发者则在自己的网站上或者论文里与MapReduce对比，鼓吹自己的系统多么先进高效，而出现了YARN之后，则形势变得明朗：MapReduce只是运行在YARN之上的一类应用程序抽象，Spark和Storm本质上也是，他们只是针对不同类型的应用开发的，没有优劣之别，各有所长，合并共处，而且，今后所有计算框架的开发，不出意外的话，也应是在YARN之上。这样，一个以YARN为底层资源管理平台，多种计算框架运行于其上的生态系统诞生了。 Spark只是一种内存计算框架，用Scala编写，它并没有自己的文件管理系统，所以Spark一般是依附于Hadoop的HDFS。可以将Spark理解为带cache的MapReduce框架。你看Spark官网的评价： Spark is a MapReduce-like cluster computing framework designed for low-latency iterative jobs and interactive use from an interpreter. Hive是Cloudera发起的项目，一开始是为了将sql语言转换为mapreduce流程所设计，但后来开发了hive on spark，就能将Spark作为Hive的计算引擎，用类SQL语言实现计算，称为H(ive)QL。但Spark本身自己也有一套SQL查询模块，叫做Spark-sql。 SparkSQL和Hive On Spark都是在Spark上实现SQL的解决方案。Spark早先有Shark项目用来实现SQL层，不过后来推翻重做了，就变成了SparkSQL。这是Spark官方Databricks的项目，Spark项目本身主推的SQL实现。Hive On Spark比SparkSQL稍晚。Hive原本是没有很好支持MapReduce之外的引擎的，而Hive On Tez项目让Hive得以支持和Spark近似的Planning结构（非MapReduce的DAG）。所以在此基础上，Cloudera主导启动了Hive On Spark。这个项目得到了IBM，Intel和MapR的支持（但是没有Databricks）。 需要理解的是，Hive和SparkSQL都不负责计算，它们只是告诉Spark，你需要这样算那样算，但是本身并不直接参与计算。 Hbase： Hadoop database 的简称，也就是基于Hadoop数据库，是一种NoSQL数据库 一个数据仓库的架构基本就是：底层HDFS，上面跑MapReduce/Tez/Spark，在上面跑Hive，Pig。 HBase is an open-source, distributed, column-oriented database built on top of HDFS (or KFS) based on BigTable!]]></content>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
</search>
