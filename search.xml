<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[四种周期 三种杠杆]]></title>
    <url>%2F2020%2F200126-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E5%9B%9B%E7%A7%8D%E5%91%A8%E6%9C%9F%20%E4%B8%89%E7%A7%8D%E6%9D%A0%E6%9D%86%2F</url>
    <content type="text"><![CDATA[本文是投资中最简单的是第八篇读书笔记。行业轮动时机的把握很难，不仅要买的准，还要卖的准，所以难度极高，邱国鹭本人不推荐过多参与，本文也只是其对行业轮动的一种解释而已，并不代表正确性。 对于大资金来说，行业配置对总体投资收益的影响常常比精选个股更为重要。一个好的荐股逻辑包括三点：估值，这只股票为什么便宜（估值水平与同业比、与历史比；市值大小与未来成长空间比）；品质，这家公司为什么好（定价权、成长性、门槛、行业竞争格局等）；时机，为什么要现在买（盈利超预期、高管增持、跌不动了、基本面拐点、新订单等催化剂）。同样的道理，行业配置的逻辑框架也不外乎估值、品质和时机这三个要素。 估值是最容易的部分，哪个行业便宜、哪个行业贵，一目了然，只是大家都把便宜的行业当作夕阳行业而不愿意买罢了。行业的品质则稍难把握，简单地说就是好行业赚钱不辛苦、坏行业辛苦不赚钱，复杂点说也无非是行业门槛、行业集中度、行业对上下游的定价权等老生常谈的东西。本章的重点是第三个要素：行业轮动的时机。 邱国鹭认为存在四种周期 政策周期 市场周期（估值周期） 经济周期 盈利周期 在市场的不同阶段，这四种周期的演变速度和先后次序是不同的。熊末牛初，股市见底时这四种周期见底的先后次序是： • 政策周期领先于市场周期。在货币政策和财政政策放松后，市场往往在资金面和政策面的推动下进行重新估值。 • 市场周期领先于经济周期。美国历史上几乎每次经济衰退，股市都先于经济走出谷底（2001年的衰退除外）。 • 经济周期领先于盈利周期，换句话说，宏观基本面领先于微观基本面。过去70年中，美国的经济衰退从未长于16个月，但是盈利下降经常持续2~3年甚至更长。 • 熊末牛初，判断市场走势，资金面和政策面是领先指标，基本面是滞后指标。熊市见底时，微观基本面往往很不理想，不能以此作为低仓的依据。如果一定要等到基本面改善才加仓，往往已经晚了。 三种杠杆 财务杠杆：对利率的弹性 运营杠杆：对经济的弹性 估值杠杆：对剩余流动性的弹性 • 第一阶段，熊市见底时，经济仍然低迷，但是货币政策宽松，利率不断降低，常常是财务杠杆高的企业先见底。此时，某些高负债的竞争对手已经或者正在出局（破产或者被收购），剩余的企业的市场份额和定价权都得到提高。 • 第二阶段，经济开始复苏，利率稳定于低位，此时的板块轮动常常是运营杠杆高的行业领涨，因为这类企业只需销售收入的小幅反弹就能带来利润的大幅提升，基本面的改善比较显著。 • 第三阶段，经济繁荣，利润快速增长，但是股票价格涨幅更大，估值扩张替代基本面改善成为推动股价的主动力，此时估值杠杆高，有想象空间的股票往往能够领涨。 • 第四阶段，熊牛替换时，不要太在意盈利增长的确定性，而应该关注股票对各种正在改善的外部因素的弹性。所谓的改善，并不一定是指正增长，也可以是下降的速度放慢，或者下降的速度好于预期。 第一阶段，首先是利率降低，使得高财务杠杆企业的利息支出减少，提升净利润; 第二阶段，经济复苏，销售收入开始增长，因为其固定成本不变，对于运营杠杆高的企业带来很大的利润增长； 第三阶段，有想象力的热点股票能够享受到流动性溢价，估值迅速提高； 举个栗子，在2011年： 三种杠杆的轮动到目前为止似乎还是依次展开了。高财务杠杆的金融地产从2011年11月30日的“降准”之日起开始有较明显的相对收益。一些高运营杠杆的周期股（例如水泥、汽车等）也在2012年9月起领先市场3个月见底，之后大幅地跑赢了股指。随着经济见底复苏的迹象越来越明显，市场轮动明显加快。 单纯的行业轮动的时机选择是个吃力不讨好的苦差事，能做对的总是少数，必须结合估值和品质综合考量。尽管本章主要探讨的是时机选择的工具，正确的做法其实应该是淡化时机选择（要素3），把投资的立足点牢固地建立在便宜的好公司上（要素1和2），因为只有估值和品质才是投资中能够把握的事，时机的选择更多只是尽人事，听天命罢了。]]></content>
      <tags>
        <tag>投资</tag>
        <tag>读书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LightGBM + NGBoost 让模型学习到更多信息]]></title>
    <url>%2F2020%2F200108-LightGBM_NGBoost_%E8%AE%A9%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E5%88%B0%E6%9B%B4%E5%A4%9A%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[NGBoost 是吴恩达提出的一种新的机器学习算法，其中NG指的是Natural Gradient (突然发现NG也是吴恩达的姓， 不知道是不是也有这个用意~ ) ， 这个算法优势在于，它是对label的概率分布进行预测，而不像其他算法只对label值做预测。所以它需要用户提前告知label的先验分布形式，比如假设label是正态分布，那这个算法本质上是在对正态分布label的两个参数（均值，方差）进行学习，最后预测形式是给定X的情况下，输出预测label的均值和方差，相当于提供了label的概率分布情况。 这种概率性预测(Probabilistic Prediction)方法对不确定性敏感的预测任务非常有用，比如天气预报，除了告知你具体的预测值，还会给你一个范围内对应的概率密度分布。 NGBoost本质上是一种meta-learning方法，他可以套用到任何现有的机器学习模型中，但在开源出来的项目代码中，NGBoost团队只使用了逻辑回归和决策树（sklearn) 作为可选的base learner，那这显然是远远不能满足我们实际项目中的需求的，所以我对NGBoost的代码做了一些修改，使其能够支持使用LightGBM作为base_learner。 1234567891011121314151617181920class NGBoost(object): def __init__(self, Dist=Normal, Score=MLE, Base=default_tree_learner, natural_gradient=True, n_estimators=500, learning_rate=0.01, minnibatch_frac=1.0, verbose=True, verbose_eval=100, tol=1e-4): self.Dist = Dist self.Score = Score self.Base = Base self.natural_gradient = natural_gradient self.n_estimators = n_estimators self.learning_rate = learning_rate self.minibatch_frac = minibatch_frac self.verbose = verbose self.verbose_eval = verbose_eval self.init_params = None self.base_models = [] self.scalings = [] self.tol = tol self.best_val_loss_itr = None 上面是NGBoost的类定义，self.Base 默认使用default_tree_learner（sklearn的决策树模型），self.Base主要在NGBoost.fit_base()中用于调用其fit方法。12345def fit_base(self, X, grads, sample_weight=None): models = [clone(self.Base).fit(X, g, sample_weight=sample_weight) for g in grads.T] fitted = np.array([m.predict(X) for m in models]).T self.base_models.append(models) return fitted 为了使用LGB，需要重写一下fit_base, 因为lightgbm训练需要使用自己独特的Dataset格式，所以fit_base方法的需要增加训练集dataset_tr的入参。 1234567891011121314151617def fit_base(self, dataset_tr, X, grads, sample_weight=None): models = list() for g in grads.T: dataset_tr.set_label(g) f_model = lgb.train( self.lgb_param, dataset_tr, verbose_eval=True, valid_sets= [dataset_tr], valid_names = ['train'], num_boost_round = self.lgb_param.get('num_round', 1) ) models.append(f_model) fitted = np.array([m.predict(X) for m in models]).T self.base_models.append(models) return fitted 基本上上面就是最核心的修改了，当然NGBoost类最好重新写一下，避免与默认情况发生混淆，完整代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136# ngboost_local.pyimport numpy as npimport lightgbm as lgbfrom ngboost.ngboost import NGBoostfrom ngboost.distns import Bernoulli, Normal, LogNormalfrom ngboost.scores import MLEfrom ngboost.learners import default_tree_learnerfrom sklearn.base import BaseEstimatorclass NGBRegressorLGB(NGBoost, BaseEstimator): def __init__(self, lgb_param, Dist=Normal, Score=MLE, Base=default_tree_learner, natural_gradient=True, n_estimators=500, learning_rate=0.01, minibatch_frac=1.0, verbose=True, verbose_eval=100, tol=1e-4): assert Dist.problem_type == "regression" super().__init__(Dist, Score, Base, natural_gradient, n_estimators, learning_rate, minibatch_frac, verbose, verbose_eval, tol) self.lgb_param = lgb_param def dist_to_prediction(self, dist): # predictions for regression are typically conditional means return dist.mean() def fit_base(self, dataset_tr, X, grads, sample_weight=None): models = list() for g in grads.T: dataset_tr.set_label(g) f_model = lgb.train( self.lgb_param, dataset_tr, verbose_eval=True, valid_sets= [dataset_tr], valid_names = ['train'], num_boost_round = self.lgb_param.get('num_round', 1) ) models.append(f_model) fitted = np.array([m.predict(X) for m in models]).T self.base_models.append(models) return fitted def fit(self, X_tr, Y_tr, X_val = None, Y_val = None, feature_name = None, sample_weight = None, val_sample_weight = None, train_loss_monitor = None, val_loss_monitor = None, early_stopping_rounds = None, callbacks=[]): X = X_tr Y = Y_tr dataset = lgb.Dataset(X, feature_name=list(feature_name)) loss_list = [] val_loss_list = [] if early_stopping_rounds is not None: best_val_loss = np.inf self.fit_init_params_to_marginal(Y) params = self.pred_param(X) if X_val is not None and Y_val is not None: val_params = self.pred_param(X_val) S = self.Score if not train_loss_monitor: train_loss_monitor = lambda D,Y: S.loss(D, Y, sample_weight=sample_weight) if not val_loss_monitor: val_loss_monitor = lambda D,Y: S.loss(D, Y, sample_weight=val_sample_weight) for itr in range(self.n_estimators): self.iteration = itr if len(callbacks) &gt; 0: for callback in callbacks: callback(self) D = self.Dist(params.T) loss_list += [train_loss_monitor(D, Y)] loss = loss_list[-1] grads = S.grad(D, Y, natural=self.natural_gradient) proj_grad = self.fit_base(dataset, X, grads, sample_weight) scale = self.line_search(proj_grad, params, Y, sample_weight) # pdb.set_trace() params -= self.learning_rate * scale * np.array([m.predict(X) for m in self.base_models[-1]]).T val_loss = 0 if X_val is not None and Y_val is not None: val_params -= self.learning_rate * scale * np.array([m.predict(X_val) for m in self.base_models[-1]]).T val_loss = val_loss_monitor(self.Dist(val_params.T), Y_val) val_loss_list += [val_loss] if early_stopping_rounds is not None: if val_loss &lt; best_val_loss: best_val_loss, self.best_val_loss_itr = val_loss, itr if best_val_loss &lt; np.min(np.array(val_loss_list[-early_stopping_rounds:])): if self.verbose: print(f"== Early stopping achieved.") print(f"== Best iteration / VAL &#123;self.best_val_loss_itr&#125; (val_loss=&#123;best_val_loss:.4f&#125;)") break if self.verbose and int(self.verbose_eval) &gt; 0 and itr % int(self.verbose_eval) == 0: grad_norm = np.linalg.norm(grads, axis=1).mean() * scale print(f"[iter &#123;itr&#125;] loss=&#123;loss:.4f&#125; val_loss=&#123;val_loss:.4f&#125; scale=&#123;scale:.4f&#125; " f"norm=&#123;grad_norm:.4f&#125;") if np.linalg.norm(proj_grad, axis=1).mean() &lt; self.tol: if self.verbose: print(f"== Quitting at iteration / GRAD &#123;itr&#125;") break self.evals_result = &#123;&#125; metric = self.Score.__name__.upper() self.evals_result['train'] = &#123;metric: loss_list&#125; if X_val is not None and Y_val is not None: self.evals_result['val'] = &#123;metric: val_loss_list&#125; return self def save_model(self, path): import joblib joblib.dump(self, path) 最后别忘了在 __init__.py 中import新写的类： 1from .ngboost_local import NGBRegressorLGB 好了，重新安装一下就可以使用了，顺便给出使用的样例代码： 12345678910111213141516171819202122232425262728293031323334353637383940from ngboost import NGBRegressor, NGBRegressorLGBfrom ngboost.distns import Normalfrom ngboost.scores import MLEfrom sklearn.datasets import load_bostonfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import mean_squared_errorif __name__ == "__main__": X, Y = load_boston(True) lgb_param = &#123; "bagging_fraction": 0.8, "boosting_type": "gbdt", "feature_fraction": 0.8, 'lambda_l1': 1, 'lambda_l2': 1, "learning_rate": 0.1, "max_bin": 255, "max_depth": 5, "min_data_in_leaf": 16, "num_leaves": 15, "objective": "regression", "task": "train", "verbose": 1 &#125; X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2) ngb = NGBRegressorLGB(lgb_param, Dist=Normal, Score=MLE, verbose_eval=1, n_estimators=10) ngb = ngb.fit(X_train, Y_train) Y_preds = ngb.predict(X_test) Y_dists = ngb.pred_dist(X_test) # test Mean Squared Error test_MSE = mean_squared_error(Y_preds, Y_test) print('Test MSE', test_MSE) # test Negative Log Likelihood test_NLL = -Y_dists.logpdf(Y_test.flatten()).mean() print('Test NLL', test_NLL)]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[下跌后坚持买入还是及时止损]]></title>
    <url>%2F2019%2F191224-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E4%B8%8B%E8%B7%8C%E5%90%8E%E5%9D%9A%E6%8C%81%E4%B9%B0%E5%85%A5%E8%BF%98%E6%98%AF%E5%8F%8A%E6%97%B6%E6%AD%A2%E6%8D%9F%2F</url>
    <content type="text"><![CDATA[本文是投资中最简单的是第七篇读书笔记。 应该如何对待亏损股呢？止损，死扛，还是越跌越买？ 要回答这个问题，我们先回顾一下卖股票的三个理由：基本面恶化；价格达到目标价；有更好的其他投资。换句话说，价值投资买的就是便宜的好公司，所以卖出的原因就是：公司没有想象的好；不再便宜；还有其他更好更便宜的公司。这三个理由均与是否亏损无关。 许多人潜意识中把买入成本当作决策依据之一，产生了常见的两种极端行为：一种是成本线上，一有风吹草动就锁定收益；成本线下，打死也不卖。另一种是成本线上无比激进，因为赚来的钱赔了不心疼；成本线下无比保守，因为本钱亏一分也肉痛。这两种极端都是人性中的“心理账户”在作祟。 忘掉你的成本，是成功投资的第一步。全市场除了你之外，没有人知道或关心你的买入成本，因此你的成本高低、是否亏损对股票的未来走势没有丝毫影响。保罗·琼斯在判断哪些股票是失败者的时候，并不是从自己的成本，而是股价的近期高点起算的——那才是人人都看得见的参照点。 熊市末期，价格显著低于价值，常常吸引价值投资者买在底部的左侧，这时候止损就容易倒在黎明前的黑暗里。然而，不止损就有潜在的毁灭性风险的问题，不可不慎。所以，不止损是有很严格的前提条件的：必须避开各种价值陷阱；所买的股票有足够安全边际；所承担的只是价格波动的风险而非本金永久性丧失的风险。上一章和本章其实就是试图回答这样一个问题：什么条件下可以死扛，什么条件下必须止损。对投资者而言，这是个生死攸关的问题。比尔·米勒（Bill Miller）在辉煌了15年之后晚节不保，在第16年把前15年的超额收益悉数退还给市场的前车之鉴，值得每个投资者深思。]]></content>
      <tags>
        <tag>投资</tag>
        <tag>读书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[投资中的安全边际]]></title>
    <url>%2F2019%2F191224-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E6%8A%95%E8%B5%84%E4%B8%AD%E7%9A%84%E5%AE%89%E5%85%A8%E8%BE%B9%E9%99%85%2F</url>
    <content type="text"><![CDATA[本文是投资中最简单的事第六篇读书笔记，投资是一个概率游戏，寻找安全边际大的企业就是在提高投资的胜率。 管理投资风险的一个重要方法，就是寻找有安全边际的公司。有安全边际的公司通常具有以下几个特点。 1.东方不亮西方亮，给点阳光就灿烂 有个段子说做豆腐最安全：做硬了是豆腐干，做稀了是豆腐脑，做薄了是豆腐皮，做没了是豆浆，放臭了是臭豆腐。未来的多种情景中，只要一种实现就能赚钱，东方不亮西方亮，这就是安全边际。对未来要求太高的股票是没有安全边际的，正如《美国士兵守则》所说，必须组合使用的武器一般都不会被一起运来。2010年买工程机械时，我心里想着：机械替代人工、保障房、城镇化、产业升级、产业转移、产能扩张、中西部大开发、进口替代、国际化、走出去战略，哪一条能实现对工程机械都是利好，这就是东方不亮西方亮的安全边际。 低估是最重要的安全边际，但需要注意这时候的低价不是因为企业真实价值受损。 2.估值低到足以反应大多数可能的坏情况 低估值是安全边际的重要来源。未来总是不确定的，希望越高，失望越大。低估值本身反应的就是对未来的低预期。只要估值低到足以反应大多数可能的坏情况，未来低于预期的可能性就很小了。很多人说，高风险高回报，低风险低回报。其实，低估值带来的安全边际是获得低风险高回报的最佳路径。价值1块钱的公司，5毛钱买入，即使后来发现主观上对公司的估值出了30%偏差，或者客观上公司出了意外导致价值受损30%，两种情况下该公司仍值7毛钱，投资者仍不吃亏，这就是低估值带来的安全边际。 限制下跌空间，和低估有点像，但其实说的是这个公司在极端情况下值多少钱。 3.有“冗余设计”，有“备用系统”来限制下跌空间 安全边际好比工程施工中的冗余设计，平日看似冗余，灾难时才发现不可或缺，例如核电站仅有备用发电系统是不够的，最好还能有备用的备用。现实生活中百年一遇的灾害可能十年发生一次，股市更是如此。铤而走险虽然能在许多时候增加收益，但是某天你会发现“出来混，迟早是要还的”。零乘以任何数都是零，所以特别要警惕毁灭性风险。垃圾股如果没有更大的傻瓜接下“接力棒”，股价是没有“备用系统”支撑的。博傻游戏玩久了，骗子越来越多，傻瓜就不够用了，还不如在低估值和基本面的双重保险中寻找安全边际。 4.价值易估，不具反身性，可越跌越买 有安全边际的公司通常业务简单，价值易估，不具有反身性。索罗斯所说的反身性是指股价下跌本身对公司基本面有负面作用，易形成自我强化的恶性循环。例如贝尔斯登股价跳水会导致交易对手“挤兑”，有反身性，故不能越跌越买；可口可乐股价跳水丝毫不影响它卖饮料，无反身性，故可越跌越买。]]></content>
      <tags>
        <tag>投资</tag>
        <tag>读书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[成长陷阱的种类]]></title>
    <url>%2F2019%2F191216-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E6%88%90%E9%95%BF%E9%99%B7%E9%98%B1%2F</url>
    <content type="text"><![CDATA[本文是《投资中最简单的事》第五篇读书笔记。 本篇主要讨论成长股常见的成长陷阱，价值股如果盈利不再具有不可持续性，那么便是价值陷阱，而成长股如果成长不可持续，那么便是成长陷阱。具体的成长陷阱有以下几类: 估值过高： 是最常见的成长陷阱，高估值的背后是高预期。对未来预期过高是人之本性，然而期望越高，失望越大。统计表明，高估值股票业绩不达预期的比例远高于低估值股票。一旦成长预期不能实现，估值和盈利预期的双杀往往十分惨烈。 技术路径踏空：成长股经常处于新兴行业中，而这些行业（例如太阳能、汽车电池、手机支付等）常有不同技术路径之争。即使是业内专家，也很难事前预见哪一种标准会最终胜出。这种技术路径之争往往是你死我活、赢家通吃的，一旦落败，之前的投入也许就全打了水漂，这是最残酷的成长陷阱 无利润增长 上一轮互联网泡沫中，无利润增长大行其道，以烧钱、送钱为手段来赚眼球。如果是客户黏度和转换成本高的行业（例如C2C、QQ），在发展初期通过牺牲利润实现赢家通吃，则为高明战略；如果是客户黏度和转换成本低的行业（例如B2C电商），让利带来的无利润增长往往不可持续。 成长性破产 即使是有利可图的业务，快速扩张时在固定资产、人员、存货、广告等多方面也需要大量现金投入，因此现金流往往为负。增长得越快，现金流的窟窿就越大，极端情况导致资金链断裂，引发成长性破产，例如拿地过多的地产商和开店过快的直营连锁（特别是未上市的）。 盲目多元化 有些成长股为了达到资本市场预期的高增长率，什么赚钱做什么，随意进入新领域、陷入盲目多元化的陷阱，因此成长投资要警惕主业不清晰、为了短期业绩偏离长期目标的公司。当然，互补多元化（例如长江实业、和记黄埔）和相关多元化（横向完善产品线和纵向整合产业链）的公司另当别论。 树大招风 要区别两种行业，一种是有门槛、有先发优势的，成功引发更大的成功；另一种是没门槛、后浪总把前浪打死在沙滩上的，成功招致更多的竞争。在后一种行业中，成长企业失败的原因往往就是太成功了，树大招风，招来太多竞争，蜂拥而至的新进入者使创新者刚开始享受成功就必须面对无尽跟风和山寨。例如团购，由于门槛低，稍有一两家成功，一年内中国就有3 000家团购网站出现，谁也赚不到钱。即使是有门槛的行业，一旦动了行业老大的奶酪引来反击，一样死无葬身之地，例如网景浏览器（Netscape）的巨大成功引来微软的反击，最后下场凄凉。 新产品风险 成长股要成长，就必须不断推陈出新，然而新产品的投入成本是巨大的，相应的风险是巨大的，收益却是不确定的。强大如可口可乐，也在推新品上栽过大跟头。稳健的消费股尚且如此，科技股和医药股在新产品上吃的苦头更是不胜枚举。科技股的悲哀是费了九牛二虎之力开发出来的新产品常常不被市场认可，医药股的悲哀则是新药的开发周期无比漫长、投入巨大而最后的成败即使是业内专家也难以事前预知。 寄生式增长 有些小企业的快速增长靠的是“傍大款”，例如有的是为苹果间接提供零部件，有的是为中国移动提供服务，在2010年的“中小盘结构性行情”中鸡犬升天，又在2011年跌回原形。其实，寄生式增长往往不具持续性，因为其命脉掌握在“大款”手中，企业自身缺乏核心竞争力和议价权。有些核心零部件生产商在自己的领域内达到寡头垄断地位，让下游非买不可，提高自己产品的转换成本让下游难以替换，或者成为终端产品的“卖点”（如英特尔），那些事实上已经具备核心竞争力和议价权、成为“大款”的另当别论。 强弩之末 许多所谓的成长股其实已经过了成长的黄金时期，却依然享有高估值，因为人们往往犯了过度外推的错误，误以为过去的高成长在未来仍可持续。因此，买成长股时，对行业成长空间把握不当、对渗透率和饱和率跟踪不紧就容易陷入成长陷阱而支付过高估值。 会计造假 价值股也有这个陷阱，但是成长股中这个问题更普遍。一个是市场期望50%增长的成长股，另一个是市场期望10%增长的价值股，哪个更难做到？做不到时，为避免戴维斯双杀，50倍市盈率的成长股和10倍市盈率的价值股，哪一个更有动力去“动用一切手段”来达到市场的预期？]]></content>
      <tags>
        <tag>投资</tag>
        <tag>读书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[价值陷阱]]></title>
    <url>%2F2019%2F191215-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E4%BB%B7%E5%80%BC%E9%99%B7%E9%98%B1%2F</url>
    <content type="text"><![CDATA[本文是《投资中最简单的事》第四篇读书笔记。 邱国鹭提出了两种投资中的陷阱：针对价值股的价值陷阱和针对成长股的成长陷阱，本篇讲价值陷阱。 价值投资最需要的是坚守，最害怕的是坚守了不该坚守的。金融危机时花旗从55元跌至1元的过程就深度套牢了无数盲目坚守的投资者。关键是要避开价值陷阱。所谓价值陷阱，指的是那些再便宜也不该买的股票，因为其持续恶化的基本面会使股票越跌越贵而不是越跌越便宜。 有几类股票容易是价值陷阱。 第一类是被技术进步淘汰的。这类股票未来利润很可能逐年走低甚至消失，即使市盈率再低也要警惕。例如数码相机发明之后，主业是胶卷的柯达的股价从14年前的90元一路跌到现在的3元多，就是标准的价值陷阱。所以价值投资者一般对技术变化快的行业特别谨慎。 第二类是赢家通吃行业里的小公司。所谓赢家通吃，顾名思义就是行业老大老二抢了老五老六的饭碗。在全球化和互联网的时代，很多行业的集中度提高是大势所趋，行业龙头在品牌、渠道、客户黏度、成本等方面的优势只会越来越明显，这时，业内的小股票即使再便宜也可能是价值陷阱。 第三类是分散的、重资产的夕阳行业。夕阳行业，意味着行业需求不再增长；重资产，意味着需求不增长的情况下产能无法退出（如果退出，投入的资产就会作废）；分散，意味着供过于求时行业可能无序竞争甚至价格战。因此，这类股票的便宜是假象，因为其利润可能将每况愈下。 第四类是景气顶点的周期股。在经济扩张晚期，低市盈率的周期股也常是价值陷阱，因为此时的顶峰利润是不可持续的。所以周期股有时可以参考市净率和市销率等估值指标，在高市盈率时（谷底利润）买入，在低市盈率时（顶峰利润）卖出。另外，买卖周期股必须结合自上而下的宏观分析，不能只靠自下而上选股。 第五类是那些有会计欺诈的公司。但是这类陷阱并不是价值股所特有的，成长股中的欺诈行为更为普遍。 这几类价值陷阱有个共性：利润的不可持续性，因此，目前的便宜只是表象，基本面进一步恶化后就不便宜了。]]></content>
      <tags>
        <tag>投资</tag>
        <tag>读书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[投资的三个基本问题]]></title>
    <url>%2F2019%2F191212-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E6%8A%95%E8%B5%84%E7%9A%84%E4%B8%89%E4%B8%AA%E5%9F%BA%E6%9C%AC%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[本文是《投资中最简单的事》第三篇读书笔记。 如果把我过去十几年的投资分析方法做一个简单的概括，最根本的就是要回答三个问题：为什么认为一家公司便宜，为什么认为一家公司好，以及为什么要现在买。这三个问题中，第一个是估值的问题，第二个是公司品质的问题，第三个是买卖时机的问题。 三个问题以难易程度排序，最难的是买卖时机，其次是品质，最容易的是估值。 首先看估值，说他容易是因为判断一个公司是不是太贵是比较简单的，但是要做到准确估值可是非常难的，所以只是从判断大致方向上，估值相对来说比较简单。估值很重要，但不是说估值每时每刻都再发挥作用，只是长期来看很重要。 便宜 现在很多人会说估值不重要，因为根据2013年的行情，5倍市盈率的涨不过50倍市盈率的，谈估值就输在起跑线上了。这种情况很正常，每几年就会发生一次。但即使是最正确的投资方法，也不可能每年都有效。所谓正确的方法，是在10年中可以有6~7年帮助你跑赢市场；而错误的方法就是在10年中只能有3~4年能跑赢市场。如果你想每年都跑赢市场，就必须不停地在不同的方法之间切换，但是，要事前知道什么时候适用哪一种方法其实是非常难的。还不如找到一种正确的方法，长期地坚持下来，这样一来，即使短期会有业绩落后的阶段，但是长期成功的概率较大。世界上每个成功的投资家都是长期坚持一种方法的，那些不断变换投资方法的人最终大多一事无成。 世界上不存在每年都有效的投资方法。一个投资方法能长期有效，正是因为它不是每一年都有效。如果一种投资方法每年都有效，这个投资方法早就被别人套利套光了。 正如乔尔·格林布拉特（Joel Greenblatt）所说，第一，价值投资是有效的；第二，价值投资不是每年都有效；第二点是第一点的保证。正因为价值投资不是每年都有效，所以它是长期有效的 巴菲特对估值考虑的不算太多，而是非常重视公司的品质。但是在A股投资，公司的品质扑朔迷离，可能比美国更难判断，所以还是买的便宜最重要。 投资肯定是讲性价比，但我认为A股很难找到高品质的伟大公司，所以一定不要为普通公司付太贵的价钱。 条条大路通罗马，不存在谁好谁坏，我只是更重视统计数据。你可以看看全世界的统计数据，10个国家中有9个国家是价值股跑赢成长股的，而且跑赢是在什么时候呢？就是在季报公布后的那两三天。比如价值股一年能够跑赢成长股7个百分点，在美国这7个百分点基本就在8个交易日内实现70%——就在每次季报公布之后的两天。这说明在季报公布出来的业绩中，成长股很容易低于预期，而价值股很容易超出预期，因为未来不会有乐观者想象的那么好，也不会有悲观者想象的那么差。成长股的成长比价值股快，但没有大家预期的那么快。 第二点看品质，品质需要花大力气研究，品质不仅仅是管理层的道德水平，还包括行业格局和护城河。邱国鹭认为对品质的判断首先是看是不是一个好行业，第二是看公司有没有差异化竞争。 品质 品质肯定是更重要的，那我为什么反复强调“便宜是硬道理”？这是因为估值方法容易，每个人都可以学。便宜不便宜大多数人都能够判断，因此关键的区别在于搞清楚公司的品质。关于时机，我不能够判断，但是绝大多数人也不能够判断。所以说，三个要素中，投资者真正需要下大力气搞清楚的就是品质。 我认为选一个好行业是成功投资的基本条件。你会发现有一些很好的管理层，很好的公司存在于烂行业中，最终也没戏。我几年前调研过几个钢铁公司，里面有的管理层很好、产品线很好、技术也很先进，但可惜在中国钢铁这么一个烂行业里，再好的管理层也无用武之地，这是行业格局使然，所谓格局决定结局。中国的钢铁行业要比美国分散得多，美国钢铁行业最后只剩下三四家在玩了，中国钢铁行业还有几十家，竞争过于激烈。我很重视行业的竞争格局，行业里一旦玩的人多了，日子就难过。玩的人不多，日子就不会差到哪里去。 当然这个不包括一些“中字头”的央企，它们的垄断是国家给的。国家授予的垄断意味着它的定价权受到政策限制。我们并不喜欢垄断本身，我们喜欢的是垄断带来的定价权，所以定价权受限制的垄断没有意义。 关于行业重要还是管理层重要： 我们很多的卖方报告过多地关注动态的信息，而对公司静态的信息分析得不够。静态的信息是什么呢？最简单的就是先回答一个问题：这家公司做的是不是一门好生意？好生意就是容易赚钱的生意。比如茅台，这个公司的商业模式很简单，哪怕现在被政府这样子打压一样能赚钱，只是增速下来了。你说他的管理层一定比钢铁公司的管理层高明很多吗？那也很难说。 所以说，马和骑师，选马比选骑师重要。特别是对大公司而言，这是一匹怎样的马比这是一个怎样的骑师更重要。特别是在中国这种环境中，你很难判断骑师的好坏。可能资深的研究员、基金经理能够和高管沟通得多一些，但大多数人可能和高管都见不了几次面。而且就算见到了高管，你有精力了解他们的中层管理人员吗？ 当然，对中小企业来说，管理层的重要性不容忽视。中国有很多小企业，看起来像是伟大的企业，但是发展到一定程度就发展不下去了，因为它受限于董事长、大股东个人的素质和境界。很多中国的上市企业在成长到100亿、150亿以后上不去，是因为管理层没有足够的眼光和胸襟。公司在不同的阶段，它的品质的看点是不一样的。小公司当然是骑师更重要，大公司就是机制和文化更重要。所以，是选骑师、选马还是选赛道，要看公司处于哪个发展阶段 差异化和同质化竞争的区别： 对品质的判断，第二个要注意的是差异化竞争和同质化竞争的区别。为什么钢铁业赚不到钱？因为提供的是同质化产品。为什么航空业在国外长期赚不到钱（在国内可以赚钱，因为大的只有3家，而美国有多家）？因为提供的基本上是无差别产品（当然细究之下还是有一些差别的，例如航空公司的历史安全纪录、机型等）。为什么白酒好赚钱？因为它是一个差异化的产品。可口可乐好赚钱，也因为它是一个差异化产品。差异化的第一个标志是品牌。 品牌也要进行区分，有的品牌只有知名度，没有美誉度。中国有很多品牌，但真正有美誉度的品牌不多。一个品牌的产品如果是用来请客送礼的就特别好，因为有个面子问题。请客送礼的东西是越贵越买的，在家里自己用的东西就不是越贵越买的。最近服装股和家纺股跌得一样惨，但其实服装还是比家纺要好。当然家纺的格局比服装好，家纺就只有三家慢慢脱颖而出了，服装竞争更激烈。但是，服装是穿在外面的，有个品牌，有个小logo，消费者就会觉着物有所值。请客送礼或者在外面可以炫耀的产品更有定价权。所以说，差异化、定价权的一个来源是品牌，而且最好是请客送礼的品牌。 差异化的第二个标志是有回头客，即用户黏度高。少量多次的购买是最好的。我不太喜欢靠大订单的企业，今年有大订单，可能明年后年就没有了。 差异化的第三个标志是单价不要太高。单价高的商品，消费者对价格较敏感，相反，单价低是个优势，卖家容易有定价权，比如口香糖，因为单价低，消费者对价格差异不敏感，它的定价权就比汽车定价权要高。汽车的品牌还是有一点用的：同样质量的车，如果有品牌，会卖得贵一点。但它不是决定性的，因为汽车是大宗商品，大家买的时候会慎重，会考虑性价比。全世界市值最大的汽车公司是丰田，而不是宝马和奔驰。可以看到，虽然宝马卖得贵得多，但是曾有很长的一段时间，丰田的净利润率超过宝马，说明汽车业更多是靠规模效应和精细化管理，宝马的品牌溢价还不足以带来比丰田更高的利润率。 差异化的第四个标志是转换成本。比如软件，前阵子公司技术部门建议我换一个大屏幕下面带两个小屏幕的新电脑，我没同意，一方面是因为我不愿意有更多的屏幕来关注短期的股价波动，但更重要的一方面是时间上的转换成本。如果更换医疗使用的设备或耗材，医生就需要时间去适应新的产品，转换成本就会高一些。转换成本高的产品用户黏性高，定价权就高。 差异化的第五个标志是服务网络。工程机械在全世界每个国家都只有1~3家，都是赢家通吃，很重要的一点就在于服务网络。比如如果一辆泵车坏了，工地停工一天就要浪费几十万的成本。所以必须要在几小时之内修好，修不好的话就要赶快拉一台新的来换。这种情况下服务网络就很重要，规模效应就很明显，龙头企业在服务布点上的优势就让后来者很难赶超。 差异化的第六个标志是先发优势。好的行业里，领先企业通常有较为明显的先发优势。我比较喜欢买的是龙头，而且是有先发优势的龙头。只要在行业内领先，后面的公司一辈子也追不上，这就是先发优势。尽管美国的科技股是美国过去30年涨得最好的行业，出了不少涨幅几百倍甚至上千倍的牛股（20世纪90年代雅虎涨了几百倍、思科涨了上千倍），但为什么长期业绩好的投资大师如巴菲特、芒格、索罗斯、彼得·林奇、约翰·内夫（John Neff）、杰里米·格兰桑（Jeremy Grantham），没有一个爱投科技股？一个重要的原因就是因为这个行业技术变化太快，先发优势不明显，护城河每3~5年就要重新挖一次，太难把握 总结一下，判断一个公司所在的行业好不好，首先是看行业竞争格局是不是清晰，领先者有没有品牌的美誉度，领先者产品的售价是不是显著高于其他竞争者，领先者有没有网络服务的优势，有没有规模效应，产品的销售半径是不是相对比较小（不用参与全球竞争），是不是有回头客，是不是低单价（下游对价格不敏感），是不是转换成本高，领先者是不是有先发优势，技术变化是不是没有那么快。对品质的评判有很多指标，核心是“这是不是一门好生意，有没有定价权，是不是一门容易赚钱的生意”。 搞清楚行业逻辑才是根本。 我分析公司、分析行业，更看重的是行业的内在特质和公司的长期经济特征这一类静态的信息，这是些规律性的东西，只有对行业真正理解了才能够说得出来。然而市场短期总是更关心公司这个月订单怎么样，下个月有没有资产注入这类动态的消息。跟踪动态消息的办法是术，不是道，若要真正长期地取得超额收益，必须要把行业的特性、内在的规律搞清楚。 一个成功的投资者应该能够把行业到底竞争的是什么说清楚，把这个行业是得什么东西得天下弄明白。比如说高端酒是得品牌者得天下，中低端酒得渠道者得天下。中低端酒的品牌忠诚度没有那么高，就看谁的渠道铺得更广，管理得更精细。同样是白酒行业，在不同的细分子行业中竞争的东西也是不一样的，投资之前必须把每个细分子行业中决定胜负的因素研究清楚。即使是同一个行业，投资者也得明白在不同阶段，到底什么因素是决定胜负的关键。比如说电子行业在一些阶段可能是得良品率者得天下，或者是得技术路线者得天下，或者是得订单者得天下。 投资微论要认识一个行业，不妨做一道填空题：得 者得天下，用一个词来概括这个行业竞争的是什么。例如，基金业是得人才者得天下，高端消费品是得品牌者得天下，低端消费品是得渠道者得天下，无差异中间品是得成本者得天下，制造业是得规模者得天下，大宗品是得资源者得天下。 最后是时机选择： 时机 时机短期择时无法实现，难度太大，相比之下长期择时更有意义，邱国鹭给出了三种方法：第一种办法是看估值。在世界各国的股市历史中，市场估值是长期均值回归的，例如，美国、欧洲的长期市盈率中值都在15倍左右。低估值时高仓位，高估值时低仓位，这个“笨办法”虽然既不能保证“总是对”、也不能保证“马上对”，但是长期坚持下来，一定是有超额收益的。第二种办法，是根据各种指标之间的领先和滞后关系进行分析。（具体请参读本书第10章）第三种办法，是根据对市场情绪的把握和逆向思考进行分析。其实，长期选时如果能够避免追涨杀跌，避免受众人的极端情绪影响，就能先立于不败之地。*历史上的股市见底信号1.市场估值在历史低位；2.M1见底回升；3.降存准或降息；4.成交量极度萎缩；5.社保汇金入市；6.大股东和高管增持；7.机构大幅超配非周期类股票；8.强周期股在跌时抗跌，涨时领涨；9.机构仓位在历史低点；10.新股停发或降印花税。]]></content>
      <tags>
        <tag>投资</tag>
        <tag>读书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[便宜才是硬道理]]></title>
    <url>%2F2019%2F191210-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E4%BE%BF%E5%AE%9C%E6%89%8D%E6%98%AF%E7%A1%AC%E9%81%93%E7%90%86%2F</url>
    <content type="text"><![CDATA[本文是《投资中最简单的事》第二篇读书笔记 市场有这样一个特点：每次上涨以后，好像乐观的观点和乐观的人就多了一些，每次下跌以后，悲观的观点和悲观的人就多了一些。而且市场上涨以后乐观的观点显得特别有深度，特别有魄力，特别有远见；下跌以后悲观的观点则显得特别睿智，特别深刻，特别有说服力。 想起在《the laws of trading》这本书里讲到了金融市场的各种既得利益方： 投资经理：靠高深的理论来掩盖长期提供不了超额收益的现实； 经纪商：指着佣金吃饭，忽悠你佣金不重要，鼓励你频繁交易； 金融媒体：每次都事后诸葛亮，告诉你昨天的市场涨跌是ABCD原因，好让公众持续关注市场。 投资中影响股价涨跌的因素是无穷无尽的，但是最重要的其实只有两点，一个是估值，一个是流动性 估值决定价格是贵了还是便宜，流动性决定了涨跌的时间。2019年的A股整体估值非常合理，而未来利率将不断下降，这会给A股带来新增的流动性，但是这需要时间，正如邱国鹭在2012年说: 现在我们对市场做一个基本的判断，应该说估值不高所以是有上涨空间的，但是大家却没有看到流动性的显著改善。虽然央行已经降低了存款准备金，但是经过了一年多的时间，大家仍然感觉资金比较紧张。流动性的改善需要时间，也就是说政策从预调、微调，到最终能够成为市场向上的推力，这个转变也需要时间。 我一直跟很多人讲长期投资如何如何，但得到的反馈是“长期来说我们都死了”。投资行业中很多人说，A股已经10年不涨，日本更是20年跌了80%，这种长期没赚到钱是为什么呢？这是因为在你买的时候股票估值已经很高了，2001年A股的市盈率是50倍，1990年日本的市盈率是70倍，如果你当时以10倍的市盈率购买，也就是以当时日经指数40 000点的1/7买入，那么成本在日经指数6 000点。日本股市经历了垮掉的20年和这么多的负面消息，现在日经指数还有8 000多点，说明只要买的时候足够便宜，就不用担心卖的时候卖不出去。 投资的胜负手很大程度在买入的时候已经决定了。上证综指近十年来在3000点附近来回震荡，看似没有长进，但实际上平均PE从30下降至13。 只要买的时候不是太贵，就不用担心卖不出去。如果你买的时候就特别贵，将来要卖出时就必须要找到一个比你更大的傻瓜，但是这个世界的特点是骗子越来越多，傻瓜不够用，不能老是指望别人当傻瓜。 关于如何选股票，其实就两点，有门槛（所以有定价权）和有积累： 选股票，一定是先选行业。就像买房子，一定是先看社区，社区不行，房子再漂亮也不行。买股票也是，股票本身再好，只要这个行业不好，一样很难涨起来。买房子先选社区，买股票先选行业，那么什么样的行业是好行业呢？很简单，有门槛、有积累、有定价权的那种行业。 有门槛，可以是牌照，可以是品牌优势，或者某种专利，或者掌握某种矿产或资源。 &gt;定价权的来源，基本上要么是垄断，要么是品牌，要么是技术专利，要么是资源矿产，或者相对稀缺的某种特定的资产。这样的行业就会有一定的定价权、一定的门槛，这样才能把竞争堵在门口，才会有积累。 有积累但是有的行业因为技术变化太快而很难有积累，你也许积累了很久，拼命挖了很深的护城河，人家可能不进攻这个城，绕了过去又建了新城。最明显的就是高科技行业，电子、科技、媒体和通信技术更新换代太快了。再看空调跟电视，空调比较持续，电视就不持续，因为电视技术老是变，以前我们看的是CRT的，后来变成DLP的，后来又变成LCD，接着是等离子，然后又变成LED，现在又变成3D。每两三年就更新换代一次，这样的企业很辛苦，要不要投资更新换代？如果不投，别人超过你，你的品牌就会受损，消费者也不买你的产品；如果投，投20亿、30亿甚至100亿只能做个两年。技术变化快的行业就是这样辛苦，而像可口可乐，一个配方可以一两百年不变。中国也有很多传统的东西可以几百年不变，这种不变的东西，反而能够有积累，他的长期回报更值得期待。]]></content>
      <tags>
        <tag>投资</tag>
        <tag>读书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何做逆向投资]]></title>
    <url>%2F2019%2F191209-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-%E9%80%86%E5%90%91%E6%8A%95%E8%B5%84%2F</url>
    <content type="text"><![CDATA[本文是《投资中最简单的事》第一篇读书笔记。 近期在读各类价值投资大师的书，股票市场每天涨涨跌跌，看似赚钱机会到处都有，但是从来没有人靠技术分析能持续赚钱的，本质上还是因为短期来看，股市涨跌与公司本身可以无关，这时候预测股价就和预测双色球一样，但长期来看，股票价格和公司一定有关，这时候就有了一定的可预测性，也因为这一个性质，价值投资是唯一一个可以成功指导股市投资的大类方法，这一领域也在持续不断的产生投资大师。 最近在读这本邱国鹭的经典价投著作，里面的很多的话值得细细咀嚼，而且常读常新，所以分主题对其进行分享和讨论。 价值投资者对一只股票的成功投资离不开两点：好公司和好价格。一般的事件和宏观因素并不足以让一个好公司的股票产生非常大的回撤，往往只有当公司或者整个行业陷入到逆境中才会产生巨大的下跌。这时候价值投资如果做出正确的逆向投资，会产生非常丰厚的回报。 逆向投资是最简单也最不容易学习的投资方式，因为它不是一种技能，而是一种品格——品格是无法学习的，只能通过实践慢慢磨炼出来。投资领域的集大成者大多数都具有超强的逆向思维能力，尽管他们对此的表述各不相同。乔治·索罗斯说：“凡事总有盛极而衰的时候，大好之后便是大坏。”约翰·邓普顿说：“要做拍卖会上唯一的出价者。”查理·芒格说：“倒过来想，一定要倒过来想。”卡尔·伊坎说：“买别人不买的东西，在没人买的时候买。”巴菲特说：“别人恐惧时我贪婪，别人贪婪时我恐惧。” 首先，看估值是否够低、是否已经过度反映了可能的坏消息。估值高的股票本身估值下调的空间大，加上这类股票的未来增长预期同样存在巨大下调空间，因此这种“戴维斯双杀”导致的下跌一般持续时间长而且幅度大，刚开始暴跌时不宜逆向投资。2011—2012年，A股计算机行业的许多“大众情人”在估值和预期利润双双腰斩的背景下持续下跌了70%就是例证。2012年年底，这些股票从成长股跌成了价值股，反而可以开始研究了。 其次，看遭遇的问题是否是短期问题、是否是可解决的问题。例如，零售股面临的网购冲击、新建城市综合体导致旧有商圈优势丧失、租金劳动力成本上涨压缩利润空间等问题就不是短期能够解决的，因此其股价持续两年的大幅调整也是顺理成章的。不过，现在大家都把零售当作夕阳行业，反而有阶段性反弹的可能——尽管大的趋势仍然是长期向下的。 最后，看股价暴跌本身是否会导致公司的基本面进一步恶化，即是否有索罗斯所说的反身性。贝尔斯登和雷曼的股价下跌直接引发了债券评级的下降以及交易对手追加保证金的要求，这种负反馈带来的连锁反应就不适合逆向投资。中国的银行业因为有政府的隐性担保（中央经济工作会议指出“坚决守住不发生系统性和区域性金融风险的底线”），不存在这种反身性，因此可以逆向投资。 逆向投资最让人不爽的一点是，往往是抄底抄在半山腰。这对于投资者是非常难以掌控的，因为合格的价值投资者只知道一只股票最终会涨，但不知道什么时候涨，类似的，他也知道这只股票最终会止跌，但不知道什么时候止跌。目前市场上比较常见做法是在下跌阶段分批次买入法，如果在没有全部买完时，股票就已经开始反弹，那就作罢，接受只赚这么多。这其实是接受了自己没有择时能力的最优解，就像大宗交易用TWAP来减少交易成本一样。 有些股票，你有持仓，但是下跌时你心里一点也不慌，甚至希望它多跌一点好让你加仓，这说明你对该股票已有足够了解，对其内在价值和未来前景有比市场更为精准的把握，因此市场价格的波动已经不会影响到你的情绪了。对这些股票而言，下跌只是提供一个更好的买点罢了——买之后的淡定，源自买之前的分析。2012年的白酒股因为塑化剂事件大幅跳水，在面对其他类似食品安全事件的逆向投资机会时，投资者可以思考这样几个问题： • 有无替代品，若有替代品（例如三株口服液之类的营养品就有众多替代品），则谨慎，若无替代品，则积极； • 是个股问题还是行业问题，如果主要是个股问题，则避开涉事个股，重点研究其竞争对手，即使是行业问题（例如毒奶粉），也可关注受影响相对较小的个股； • 是主动添加违规成分还是“被动中枪”，前者宜谨慎，后者可积极； • 该问题是否容易解决，若容易解决，则积极，若难以解决（例如三聚氰胺问题），影响可能持续的时间长且有再次爆发的可能性，则谨慎； • 涉事企业是否有扎实的根基，悠久的历史传承和广泛的品牌美誉度在危机时刻往往有决定性的作用，秦池、孔府的倒台就是由于根基不稳而盘子却铺得太大； • 是否有突出的受害者个例，这决定了事件对消费者的影响是否持久。 BREAKING NEWS是机会，人们总是对爆炸性新闻关注过多，却对大的历史趋势习以为常，在美国911事件后买入航空股的人都获利颇丰，7·23”甬温线特别重大铁路交通事故后的一两个月购买铁路建设和铁路设备的股票的投资者，也大幅跑赢市场一年多。 其实还有今年7月3日的新城控股董事长猥亵案，如果能够在最低点买入，已经能够赚50%了。但同样，这些黑天鹅对股票的影响往往是一个月以上，所以迫不及待的贸然抄底显然会非常容易出现抄底抄在半山腰的情况，对于这一点，我并没有好的解决方法，邱国鹭也没有给出好的解决方法。 其实，独立思考、逆向而动效果往往更好。基金公司作为一个整体的行业配置，在一般情况下是对的（毕竟专业人士相较于其他市场参与者还是有一定优势的），但是在极端的情况下，基金公司也很可能是错的。2014年年初，在基金公司的行业配置中，对TMT（Technology，Media，Telecom，科技、媒体和通信产业）和医药的超配程度以及对金融地产的低配程度都达到了十年之最。上一次基金整体配置如此失衡是在2010年年底。2010年11月我接受《中国证券报》采访时，提到的一个论题就是“银行与医药股票哪一个前景更好”，当时我的一个基本结论就是医药比银行贵3倍，但是增长不可能比银行快3倍。在之后的两年中，2011年银行股跌了5%，医药股跌了30%，2012年银行股涨了13%，医药股涨了6%，两年累计下来看，机构一致低配的银行股大幅跑赢了机构一致超配的医药股，再一次验证了“最一致的时候就是最危险的时候”这句老话。2013年各机构再次一致地憧憬着老龄化对医药的无限需求，把医药股的估值推高到30倍市盈率。比起5倍市盈率的银行，当时机构做出的比较和得出的基本结论现在几乎可以原封不动地重复一遍。 人多的地方不去，目前A股正在吹捧核心资产，上周看到一个基金经理大肆鼓吹中证100就是核心资产，其中的核心依据就是中证100近两年比沪深300，中证500涨的好。正所谓涨了的就是核心资产，不涨就不是。邱国鹭对于这种情况，已经给出了解决方法： 坦率地说，我也不知道医药股的高估值还能持续多久，也许会从高估变成更高估。不论错误定价的程度有多大，没有人能够事前预知拐点。作为投资者，我们能分辨清楚的就是市场的错误定价在哪个板块以及错误的程度有多大，然后远离被高估的板块，买入被低估的公司。至于市场要等多久才会进行纠错，纠错前会不会把这种错误定价进一步扩大，就不是能够预测的了。 但对于抄底/止盈在半山腰的情况，邱国鹭也只是给与了一些心理按摩，并没有给出实质的解决的方法。 当然，任何投资方法都有缺陷，逆向投资的短板就是经常会买早了或者卖早了。买早了还得熬得住，这是逆向投资者的必备素质。投资者必须明白一个道理，市场中没有人能够卖在最高点、买在最低点。在2007年的牛市中，即使指数后来涨到了6 100点，能够在4 000点以上出货也是幸运的；在2008年的熊市中，即使指数后来又跌到了1 664点，能够在2 000点建仓也是幸运的。顶部和底部只是一个区域，该逆向时就不要犹豫，不要在乎短期最后一跌的得失，只要能笑到最后，短期难熬点又何妨？只有熬得住的投资者才适合做逆向投资。在A股这样急功近利的市场中，能熬、愿熬的人少了，因此逆向投资在未来仍将是超额收益的重要来源。]]></content>
      <tags>
        <tag>投资</tag>
        <tag>读书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈谈A股唯一一家生活用纸企业]]></title>
    <url>%2F2019%2F191009-%E4%B8%AD%E9%A1%BA%E6%B4%81%E6%9F%94%2F</url>
    <content type="text"><![CDATA[中顺洁柔是一家生活用纸企业，也是A股唯一一家上市的生活用纸企业，产品类型包括我们日常常见卷纸、抽纸、纸手帕、湿巾等，其主打品牌为”洁柔”。 基本情况目前中顺营业收入在60亿元左右，毛利率35%，其中营业成本的60%是纸浆成本，所以中顺理论上应该有周期股的特性，特别是2017下半年国际纸浆价格上涨了50%，2018也维持高位，但中顺依然维持了稳定的增长，可见其管理层的优秀功力。 中顺洁柔的业务非常纯粹，根据2018年报，其营业收入的98%来自于生活用纸，而A股的纸业公司普遍存在多元化经营，业务线庞杂等现象，这也凸显出中顺洁柔管理层的独特。 通过整理对比A股的造纸行业股，中顺洁柔的毛利率在整个行业中近三年一直稳居第一，基本维持在35%左右，但是净资产收益率能力差不多在12%，在整个行业中并不算最出色的。主要是因为，作为TO C的生活用纸企业，中顺的销售费用率一直在10%左右，相比很多TO B的造纸企业，显然不占优势。 中顺近三年企业经营情况表现稳健，营收每年增长20%，净利润每年增长30%，这一方面是消费股的优势，另一方面也再次说明中顺管理层的优秀，在2018以来宏观环境不佳的背景下，还能逆势取得稳定增长，而且净利润增长比营收增长快，说明各项成本的得到加速控制。 行业对比目前，国内生活用纸行业包括了恒安国际、金红叶纸业、维达国际和中顺洁柔，其中恒安国际和维达国际是港股上市企业。 恒安国际是行业第一，在经营策略上走的是多元化集团的路线，旗下有纸巾产品（中顺的直接竞品）、卫生巾产品、护肤产品和纸尿裤产品，在产品定位上横跨高中低各档产品线，收入合计超200亿，其中纸巾产品占比50%。 金红叶为金光纸业在中国的生活用纸运营主体，经营策略上的特色是纵向一体化（集团自供应原材料），于此带来的是整体上的成本优势，顺着这种优势，经营定位上是主打中低端市场+大单品策略，其中“清风”系列产品毛利润占比多年保持在总量的80%以上。 维达国际2018年营业收入合计大概在150亿左右，其中生活用纸产品占比80%，其经营策略和产品定位和中顺非常类似，因此我认为是中顺在市场上最直接的竞争对手。目前中顺年营业收入在60亿左右，基本上是维达一半的体量，但从增长率来看，这三家企业的生活用纸单项增速都在10%以下，只有中顺维持20%以上的收入增速。 估值分析中顺目前的增长和盈利已经受到市场的认可，其PETTM已经摸到了历史较高水位，达到37。同类型的维达国际在港股的市盈率为24，恒安国际市盈率为15。以目前的市盈率来看，中顺已经不便宜了，即使它是这个行业中增长速度最快的企业，其增长潜力也很大，但是增长绝对速度也只有20%，并不算是高速增长，目前市场的估值有些偏乐观了。 中顺其实还有一个隐忧。一方面消费者对生活用纸的价格是比较敏感的，企业一般很难单独做出提价，因为消费者对于品牌并没有很强的认知，依然是被价格主导。虽然中顺努力开拓品类，开发湿巾类，婴儿用纸等高利润产品，但毕竟其收入占比目前仍然较低。 另一方面，其实中国仍然有大量的四五线城镇的人，平时是不怎么用抽纸/面巾纸的。据统计，我国人均生活用纸消费量达到6.2kg，与发达地区相比，日本韩国 23-25 公斤、欧美 30 公斤、香港/台湾地区 10+公斤有显著差距。从局部情况来看，北京、上海等一线城市人均生活用纸消费量已经达到10kg，已经追上港澳地区，但四五线城市的需求显然还没有被挖掘出来。 但目前这一情况在迅速改善，但是很可惜和中顺并没有关系。不知道大家知不知道“丝飘“这个品牌？它是一个完全依靠拼多多起家的纸巾品牌，依靠29.9元买30包抽纸的爆款，目前年销售额超过4亿元，因为其不到1元一包的单价，还被动出口到越南。这些企业本身是OEM,ODM出身，在外贸订单萎缩背景下，依靠拼多多的新品牌扶持计划，迅速占领广大低线城市市场，甚至是挖掘出原来大企业不曾培育出的新市场。这些企业的产品定价非常接近成本，利润率相当之低，但只要他们把规模做上去，仍然能活的很好。所以在这个游戏里，消费者得到质优价廉的商品，工厂活了下来，拼多多也得到用户，损失的只有类似中顺的传统大企业，它丧失了获取这块市场的机会。 所以中顺依靠优秀的管理能力，相信仍然能维持稳定的增长，但远期的困境可能就在于向上突破还有点远，向下防守力不从心，结合目前的估值，可能并不是最好的入手时机。]]></content>
      <tags>
        <tag>投资</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[谈谈值得买（300785）-这家细分行业龙头还值得买吗]]></title>
    <url>%2F2019%2F191008-%E5%80%BC%E5%BE%97%E4%B9%B0%2F</url>
    <content type="text"><![CDATA[今天要谈的公司是值得买（300785）。值得买这家公司大家应该都比较熟悉，在A股这个群魔乱舞的市场，大家还是比较认可值得买，一方面是作为一家月活超2000万的电商导购平台，很多股民也是值得买的用户，况且值得买的定位就是一二线城市的男性用户，与A股投资者有天然的契合；另一方面，值得买的主营业务非常干净纯粹，就是给电商导流，暂时还没有染上A股市场一些垃圾公司喜欢搞资产重组，概念炒作的坏毛病，这点深得价投喜爱。 值得买主要产品就是smzdm.com网站和APP，公司前身是公司创始人隋国栋搭建出个人网站“smzdm.com”，为消费者收集各平台的优惠信息，筛选折扣力度最大的促销信息并通过网站发布给消费者，其收入来源主要是靠电商返佣和广告。作为互联网企业，其资产负债结构也相对简单，所以我们主要看下其利润表中的营收情况。 再加上一些基本的财务指标 初看这张利润表，我会觉得这是一家非常会赚钱的企业，毛利率在70%以上，净利率20%左右，营收增长率30%以上，所在的赛道（内容导购平台）在A股也没有竞争对手，具有稀缺性，感觉是A股少有的兼具好公司好行业的标的。 看的再仔细一点，感觉有些地方也不是那么完美：为什么2018年之后营业收入增长率从80%下降到38%，但营业成本反而加速上涨，毛利率也从83%下降到73%，毕竟是市盈率60倍的股，盈利能力下降可不是什么好消息，感觉这里面会有坑。 为了解答这个的问题，我仔细阅读了公司的招股书和最新的中报，果然，现实总是残酷的，让我一一道来。 我们首先分解报表上的营业收入，值得买将营业收入分为两类，一类是信息推广业务，其中按照收费模式的不同，分为电商导购佣金收入和广告展示收入。另一类是互联网效果营销平台服务，简单来说就是值得买成立了一个子公司做中间平台，一端连接拥有流量的网站、移动客户端和内容创业者等媒体，一端连接电商、品牌商广告主。但目前这个中间平台卖的流量主要都是值得买自己的网站和APP资源，所以本质上这部分收入和电商导购佣金性质是一样的。 所以从业务属性上来看，信息推广业务中的电商导购收入和互联网效果营销收入赚的是佣金，也就是按照CPS对商家收费，我把它叫做佣金收入，佣金收入=佣金费率GMV；而信息推广业务中的广告展示收入是按照广告位刊例价收费，一般是几万元/天，这里简称为广告收入，广告收入=刊例价广告位数量。 值得买广告刊例价（来源：招股书） 根据招股书披露的数据，我计算了各年度佣金收入和广告收入，发现基本上值得买至少一半收入是来源于广告，例如2018年的5.06亿收入中，广告收入为3.06亿，佣金收入为2亿元。 在2017，广告收入同比大幅增长了125%（来自于广告刊例价大幅提升），但在2018广告增速回落到35%（刊例价保持稳定），这直接导致营业收入增速从2017年的82%回落至38%。相比而言，佣金收入增速一直比较稳定，维持在40%左右。由于公司网站和APP端的广告位数量已不可能大幅增加，在宏观经济不景气的背景下，刊例价也没有大幅提升的可能，未来广告收入增速不太可能回到之前的超高速增长了。我预估广告收入增长会小于佣金增长，这会导致营收承压，未来增长率可能会小于30%。 通过上面的计算，目前值得买的广告收入占营收比重已达到60%，感觉值得买已经变成一个媒体公司了？这一点可能和广大用户的认知会有冲突，相信有很多张大妈的忠实用户都认为值得买是一家比较中立的消费信息聚合平台，但是当网站和APP上充斥了各种广告，这其实是很影响用户体验的。 从我个人的用户体验出发，其实现在smzdm.com上已经充满了广告，但因为它并没有像搜索引擎那样在banner下方标明这是广告，而是把自己伪装成优惠信息，很多人根本看不出来这是广告。这必然对平台的公信力和中立度有损害，别忘了值得买的用户群是理性思维能力最强的理工男，进一步提高广告收入占比损害的是平台长远利益，只有依靠挖掘优质信息，提高GMV带动佣金收入，才是平台健康的内生生长路径。 所以收入放缓的原因我们找到了，但另一个不好的现象是，营业成本依然在狂飙。2018年营业成本增长了127%，2019年中报依然增长88%，这说明新获取收入的成本越来越贵，导致其毛利率从83%下滑至72%。公司招股书对营业成本增长的解释是，用户增长和推荐内容爆炸式上升，新增了大量运营人员和IT开支。 OK，那为什么成本增长和营收增长没有匹配呢？招股书中没有解释，但我还是找到一个方法来衡量新增成本到底有没有效益。 值得买的核心优势之一就是用户贡献内容（UGC），用户将电商平台上发现好价或者原创经验发布在平台上，目前用户贡献内容占平台整体内容数量的70%，贡献收入占比也在70%左右。为了增加爆料信息时效性和覆盖面，公司在2017年上线了机器贡献（MGC），通过爬虫自动监控全网好价，然后自动生成商品描述并发布。这一块内容虽然发布条数数量多（占比17%），但是收入贡献占比很少（3.7%）。 值得买优惠内容条数和营收贡献（来源：招股书） 公司的招股书中给出了近三年值得买平台上发布的优惠内容条数和相应收入信息。基本上公司的优惠信息数量呈每年翻一番的速度增长，并且在18年由于MGC的贡献，更是增长了155%。由于用户发布内容需要大量的人工审核，因此营业成本中运营开支也产生了相应的增长，且IT费用也会成比例增加。所以2018年的营业成本增速127%，这一点和优惠信息数量增速（剔除MGC）118%是比较匹配的。 我们之前说到，2018年佣金收入只增加了46%，为什么内容数量的增加为何没有带来营收的同等大幅度增加呢？我将贡献收入/内容条数作为衡量优惠内容的带货能力的指标，这个指标实质上就是每发布一条内容能为公司赚取多少钱。 近三年，除了编辑贡献内容带货能力是稳定增长的，其余类型内容的带货能力都有不同程度下降。特别是占70%比例的用户贡献内容，从2016年的142元/条下降到2018年的64元/条。这一点从招股书披露的GMV数据也能得到验证，根据计算，每发布一条优惠内容所产生的GMV是在不断下降的：从2016年的5862元下降到2018年3445元，即使剔除掉带货能力最差的MGC，2018年的单条内容产生GMV也只有4152元。 这是一个比较严重的问题，说明UGC质量已大不如前，但带来的单位成本并没有减少，导致成本的增速远高于营收增速，另一方面，用户也对信息过载产生了疲劳，目前公司的ARPU用户平均收入是19元/年，低于腾讯（53 元/年）、爱奇艺（21 元/年）和微博（23 元/年），这个ARPU的表现不算好，且已经有未老先衰的迹象。 值得买19年中报营收是2.73亿元，按照前两季度占年收入40%计算，2019年总收入预计在6.82亿元，同比增长率34%，净利润1.22亿元，同比增长率23%，且未来三年平均增长率应该会小于20%，这已经是比较乐观的估计，显然当前的股价（市盈率60+）已经透支了未来好几年的发展。 最后，是我夹带私货的一些唠叨。值得买这个公司在国内没有完全一样的竞品，在国外其实也没有规模相若的对手（只有一家北美省钱快报产品和盈利模式类似），这是一个依附于大行业（电商）的利基市场，它能赚钱，但是有显著的天花板，所以虽然他是这个利基市场的龙头，但行业规模限制了它可能只能做到这么大，除非它拓展业务，改换赛道，但值得买又比较忌讳自己亲自下海做电商，因为它担心占自己收入来源60%+的大客户（京东+淘宝）抛弃它。未来值得买可以继续讲自己的原来的故事，只是增速必然会越来越慢，和几年前相比，现在国内有很多产品已经能部分替代值得买的产品体验，如商品评测（盖得排行），商品体验分享（知乎），高性价比平台（小米，拼多多），甚至未来这个利基市场会不会一直存在下去，或者被其他行业改变都是一个疑问。互联网行业的发展速度决定了所有在这个行业玩家必须拼命奔跑，不进则退，而值得买现在可能已经不值得买了。]]></content>
      <tags>
        <tag>投资</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[社融是什么]]></title>
    <url>%2F2019%2F190822_public-loan-component%2F</url>
    <content type="text"><![CDATA[社会融资规模(社融)指的是实体经济从金融体系拿到的钱。可以是金融机构直接给的钱，比如贷款、保险赔偿金等；也可以是金融机构帮着拉皮条搞来的钱，比如非金融企业发行股票、债券等。 虽然是中国特有的经济指标，不过社融基本上可以等价于IMF（国际货币基金组织)倡导的“信用总量”的概念。反映的是实体经济对货币的需求量。 社融统计的是境内机构和个人投资到实体经济的钱，所以不统计境外机构的投资，也不统计金融机构之间的融资，如同业拆借。 社融的组成 表内融资：人民币贷款指的是金融机构出借给非金融企业/机关团体/个人的人民币贷款(贷款合同、票据贴现、垫款、贸易融资等形式都算)。个人房贷是属于该项目下。 外币贷款和人民币贷款一样，只不过币种是外币。 人民币贷款和外币贷款都是“表内业务”，金融机构发放贷款时需要把帐记到资产负债表里头，靠利差来赚钱。 监管会时不时地来查查表，而且还要求银行需要针对这些业务上交存款准备金，还要满足资本充足率等要求 而下面的这三个则是“表外业务”。金融机构主要做的是拉皮条的活，收手续费。资产不入表，所以也没有上面提到的限制。 表外融资（非标融资）： 委托贷款A要跟B借一笔钱，利率期限什么都谈好了，但是B没有放贷资质，所以B委托银行来当个中间人。这笔放款就叫做委托贷款。 信托贷款投资者可以把钱投资到资金信托产品里头(可以简单理解为信托公司卖的基金)，信托公司会把一部分钱拿去放贷，这部分就是信托贷款。 未贴现银行承兑汇票企业A跟B做生意，A欠B钱，于是开了张欠条(汇票)给B。并且A找来了银行，银行承诺一定期限后B可以拿这张票来换钱。B为了早点拿到钱，也可以提前把这张欠条打折卖给银行(打折出售哈银行的过程披称为“贴现”)。“未贴现银行承兑汇票”这个指标指的是B还hold着、还没有打折出售的那部分汇票。 从社融存量的结构上来看的话，人民币贷款占了接近七成。 企业债券融资，非金融企业境内股票融资都比较好理解。 地方政府专项债券，该项融资主要对接公路建设、棚改等基建项目，属于向实体经济提供资金范围]]></content>
  </entry>
  <entry>
    <title><![CDATA[快速检测时间序列是否为平稳序列]]></title>
    <url>%2F2018%2F181217_%E5%BF%AB%E9%80%9F%E6%A3%80%E6%B5%8B%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%E6%98%AF%E5%90%A6%E4%B8%BA%E5%B9%B3%E7%A8%B3%E5%BA%8F%E5%88%97%2F</url>
    <content type="text"><![CDATA[对于没有学习过随机过程课的我而言，判定一个序列是否为平稳主要靠看图说话…而且机器学习跑模型，一般而言也不需要了解特征这块的性质，但最近做量化模型比较多，如果一个特征是非平稳的话，它可能会给模型一些不好的先验，这是我想避免的。所以这篇文章主要讲一下如何快速判断一个时间序列是否是平稳的。 需要了解一个概念，一般非平稳序列都有单位根(unit root)，所以可以使用单位根检验来判定一个序列是否是平稳，最常用的检验就是A.D.Fuller检验。我把上证综指的日收益率序列画出来，基本上可以看到还是比较平稳的。1234567891011import bottleneck as bnrolmean = bn.move_mean(timeseries, window=14)rolstd = bn.move_std(timeseries, window=14)#Plot rolling statistics:orig = plt.plot(timeseries, color='blue',label='Original')mean = plt.plot(rolmean, color='red', label='Rolling Mean')std = plt.plot(rolstd, color='black', label = 'Rolling Std')plt.legend(loc='best')plt.title('Rolling Mean &amp; Standard Deviation')plt.show(block=False) 下面对这个时间序列使用AD Fuller检验，statsmodels里有现成的函数可以调用。1234567891011121314151617from statsmodels.tsa.stattools import adfullerdftest = adfuller(timeseries, autolag='AIC')dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])for key,value in dftest[4].items(): dfoutput['Critical Value (%s)'%key] = value print (dfoutput) -------------------------------------------Results of Augment Dickey-Fuller Test:Test Statistic -1.266111e+01p-value 1.304351e-23#Lags Used 1.400000e+01Number of Observations Used 3.025000e+03Critical Value (1%) -3.432514e+00Critical Value (5%) -2.862496e+00Critical Value (10%) -2.567279e+00dtype: float64 上面把AD Fuller检验结果也打印出来了。一般我就看一下p-value是否小于0.05，或者看Test Statistic是否小于*Critical Value(5%)，如果小于的话，就表示这是平稳序列。]]></content>
      <tags>
        <tag>Statistics</tag>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Xgboost特征重要性的可视化]]></title>
    <url>%2F2018%2F181217_Xgboost%E7%89%B9%E5%BE%81%E9%87%8D%E8%A6%81%E6%80%A7%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[对于决策树模型而言，其相比神经网络一个优势就是还能保留一点的可解释性。当然对于GBDT树，除了第一颗树我们可以从决策树的角度来理解模型是怎么工作，后面的所有树其实都是在学习残差，所以也不是很容易直观理解了。但无论如何，我们总是可以很方便的统计所有树模型的特征的重要性，重要性的衡量依据可以是特征使用次数，也可以特征增益。 对于XGboost模型，一般使用这行代码把特征重要性打印出来1model.get_fscore() 如果是lightGBM模型，改为这行代码1model.feature_importance() 我最近在训练模型的时候，希望能够看到模型每一步/每几步迭代过程中的特征使用情况，而不只是在模型全部训练完之后看一个整体。所以就写了以下代码，能够把模型迭代过程中的特征使用情况以及变化趋势可视化地展示出来。 模型训练我们用sklearn提供的wine dataset作为我们的toy example.12345678from sklearn import datasetsimport pandas as pdimport numpy as npimport xgboost as xgbwine = datasets.load_wine()X = wine['data']y = wine['target'] 训练集，测试集分割，构造DMatrix12345from sklearn.model_selection import train_test_splitX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10)dtrain = xgb.DMatrix(data=X_train, label=y_train, feature_names=wine['feature_names'])dtest = xgb.DMatrix(data=X_test, label=y_test, feature_names=wine['feature_names']) 定义XGboost训练参数1234default_param = &#123;'booster':'gbtree', 'silent':0, 'objective':'multi:softmax', 'num_class':3, 'eta':0.1, 'max_depth':4, 'eval_metric':'mlogloss', 'tree_method':'hist'&#125;num_round = 100 因为我们需要记录模型迭代过程中特征重要性，所以需要写一个callback函数让xgboost在每一轮迭代后调用，这个函数的作用就是记录下当时模型的特征重要性。123456789def record_fscore(fscore_result): if not isinstance(fscore_result, dict): raise TypeError('fscore_result has to be a dictionary') fscore_result.clear() def call_back(env): if env.iteration % 2 == 0: #每两轮记录一次 fscore_result[env.iteration] = env.model.get_fscore() return call_back 模型训练12345fscore_result = &#123;&#125;model = xgb.train(default_param, dtrain, num_boost_round=num_round, evals=[(dtrain, 'train'), (dtest, 'test')], verbose_eval = True, callbacks = [record_fscore(fscore_result)]) 训练完成后，fscore_result是这样子的：123456789101112print (fscore_result)&#123;0: &#123;'proline': 2, 'ash': 1, 'flavanoids': 2, 'alcalinity_of_ash': 1, 'color_intensity': 2, 'alcohol': 1&#125;, 2: &#123;'proline': 6, 'ash': 1, 'flavanoids': 6,... 特征可视化特征可视化我们使用plotly,相比matplotlib的优点在于它会生成一个可交互的html页面，并且它的heatmap灵活性比matplotlib强太多了。 首先定义一个后面会用到的辅助函数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def array_map(arr, mapping, default=np.NaN, on_missing='raise', return_pandas=False): """ 把arr的值，通过给定的一个映射关系mapping，应设成mapping中的value mapping: dict/pd.Series/(key_array, value_array) default: 如果有值不存在于mapping中，fill成default on_missing: 'raise'/'fill', 默认如果有不存在的值，会抛出异常，否则用default填充 return_pandas: 如果mapping是Series或者DataFrame，是否返回Series和DataFrame（默认为False，返回ndarray） example: &gt;&gt;&gt; import numpy as np &gt;&gt;&gt; import pandas as pd &gt;&gt;&gt; arr = np.array([1, 2, 1, 3, 2]) &gt;&gt;&gt; mapping = &#123;0:1, 1:0, 2:3, 3:2&#125; &gt;&gt;&gt; array_map(arr, mapping) array([0, 3, 0, 2, 3]) &gt;&gt;&gt; array_map(arr, pd.Series([0, 1, 2, 3], index=[1, 0, 3, 2])) array([0, 3, 0, 2, 3]) """ if isinstance(mapping, tuple): arr_key = np.array(mapping[0]) arr_value = np.array(mapping[1]) if len(arr_value.shape) == 0: arr_key, arr_value = np.broadcast_arrays(arr_key, arr_value) if on_missing == "raise": idx_mapping = pd.Series(np.arange(arr_key.shape[0], dtype=np.uint32), index=arr_key) else: arr_value = np.append(arr_value, np.expand_dims(default, axis=0), axis=0) idx_mapping = pd.Series(np.arange(arr_key.shape[0], dtype=np.uint32), index=arr_key) mapped_indices = array_map(arr, idx_mapping, default=arr_key.shape[0], on_missing=on_missing, return_pandas=False) return arr_value[mapped_indices] ks, indices = np.unique(arr, return_inverse=True) if isinstance(mapping, pd.Series) or isinstance(mapping, pd.DataFrame): if on_missing == 'raise': n_idx = np.unique(mapping.index.values) n_all = np.unique(np.concatenate([ks, mapping.index.values])) if n_all.shape[0] &gt; n_idx.shape[0]: raise Exception(f"MISSING VALUE IN MAPPING: &#123;set(n_all) - set(n_idx)&#125;") ans = mapping.reindex(ks, fill_value=default).values[indices] if return_pandas: if isinstance(mapping, pd.Series): return pd.Series(ans, index=ks[indices]) elif isinstance(mapping, pd.DataFrame): return pd.DataFrame(ans, columns=mapping.columns, index=ks[indices]) else: return ans else: if on_missing == 'raise': return np.array([mapping[k] for k in ks])[indices] else: return np.array([mapping.get(k, default) for k in ks])[indices] 开始画图了12345678910111213141516from plotly.offline import plotimport plotly.graph_objs as goif not isinstance(fscore_result, dict): raise TypeError(&apos;fscore_result should be a dictionary&apos;)feature_list = dtrain.feature_namesrounds = list(fscore_result.keys())num_round = len(fscore_result)num_feat = len(feature_list)z = np.zeros((num_round, num_feat), dtype=np.int)for i, round in enumerate(rounds): z[i,:] = array_map(feature_list, fscore_result[round], on_missing=False, default=0.0001)z_prev = np.pad(z, ((1,0),(0,0)), mode=&apos;constant&apos;, constant_values=0)[:-1]z_inc = z - z_prev z_inc 即为模型每轮新增使用的特征计数，下面就是画heatmap的代码。123456789101112data= [go.Heatmap( ​ z=z_inc,​ x=feature_list,​ y=np.arange(num_round),​ colorscale=&apos;Viridis&apos;,​ )]layout = go.Layout( title=&apos;Feature Importance&apos;)fig = go.Figure(data=data, layout=layout)plot(fig, filename=&quot;feat_importance_heatmap.html&quot;) 最终生成的html如图 相比只看一个最终整体的特征重要性，观察每轮/每几轮 模型迭代使用的特征情况还是能够带来一些新信息的，我们可以看到有些特征，一开始有些用，到后期就没有用了，而有些特征则一直都很用。]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对比T检验卡方检验和F检验]]></title>
    <url>%2F2018%2F180401_T%E6%A3%80%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[T检验，卡方检验和F检验是最常见的三大统计假设检验方法，本文将主要介绍在何种情况下，如何使用某一种检验方法，但对具体的原理不做过多解释，重在实用。 T检验T检验，针对的是样本均值。假设总体均值$\mu=1.1$已知，现有一组服从正态分布的数据样本集合： 1(1) 1.0, 1.2, 1.4, 1.1, 1.3, 1.2 能否判断这组数据样本是否来自于已知总体，限定显著性水平$\alpha=0.05$. 解： 步骤一：我们选择T统计量。T统计量的计算公式如下 $$T= \frac{\hat{x}-1.1}{s/ \sqrt{n}}$$ 其中，$\hat{x}$是样本均值，$s$为样本标准差，计算公式为$s=\sqrt{\frac{1}{n-1} \sum{i=1}^{n}(x{i}-\hat{x})^2}$ 计算得到 $T=0.28867$ 步骤二：查T检验临界值表。因为样本中拥有6份数据，因此采用$n=5$（自由度为5）所对应的行；显著性水平$\alpha=0.05$，因此采用双侧检验$p=0.05$所对应的列。 可以看到，查表值为2.571， 由于$T=0.28867&lt;2.571，所以我们可以认为样本是来自于$\mu=1.1$的总体。 t检验类型 ​ t检验有多种类型，可以分为只有一组样本的单体检验和有两组样本的双体检验。单体检验用于检验样本的分布期望是否等于某个值。双体检验用于检验两组样本的分布期望是否相等，又分为配对双体检验和非配对双体检验。配对双体检验的两组样本数据是一一对应的，而非配对双体检验的两组数据则是独立的。比如药物实验中，配对双体检验适用于观察同一组人服用药物之前和之后，非配对双体检验适用于一组服用药物而一组不服用药物。 ​ 1）单体检验​ 单体检验是针对一组样本的假设检验。零假设为$\pmb{H}_0:u=u_0$,统计量 $t = \frac{\bar{x} - \mu_0}{s/\sqrt{n}}$服从自由度$n-1$的 T 分布。 ​ 2）配对双体检验​ 配对双体检验针对配对的两组样本。配对双体检验假设两组样本之间的差值服从正态分布。如果该正态分布的期望为零，则说明这两组样本不存在显著差异。零假设为 $\pmb{H}_0:u=u_0$。统计量$t = \frac{\bar{d} - \mu_0}{s/\sqrt{n}}$服从自由度为 $n-1$ 的 T 分布，其中 $\bar{d}$ 是差值的样本均值，$s$是差值的样本标准差。 ​ 3）非配对双体检验​ 非配对双体检验针对独立的两组样本。非配对双体检验假设两组样本是从不同的正态分布采样出来的。根据两个正态分布的标准差是否相等，非配对双体检验又可以分两类。一种是分布标准差相等的情况。零假设是两组样本的分布期望相等，统计量 T 服从自由度为 $n_1+n_2-2$l的 T 分布。 $$\begin{eqnarray} t &amp;=&amp; \frac{\bar{x_1}-\bar{x2}}{s{x_1,x_2} \sqrt{1/n_1+1/n2}} \nonumber\ s{x_1,x_2} &amp;=&amp; \sqrt{ \frac{(n_1-1)s_1^2+(n_2-1)s_2^2}{n_1+n_2-2} } \nonumber \end{eqnarray}$$ 其中 $\bar{x_1}]$和 $\bar{x_2}$分别是两组样本的样本均值，$n_1$ 和$n_2$分别为两组样本的大小，$s_1$ 和 $s_2$分别是两组样本的样本标准差。另一种是分布标准差不相等的情况。零假设也是两组样本的分布期望相等，统计量 T 服从 T 分布。 $\begin{eqnarray} t = \frac{\bar{x}_1-\bar{x}_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}} \nonumber \end{eqnarray}$ T 分布的自由度为 $\begin{eqnarray} d.f. = \frac{ (s_1^2/n_1+s_2^2/n_2)^2 }{(s_1^2/n_1)^2/(n_1-1)+ (s_2^2/n_2)^2/(n_2-1)} \nonumber \end{eqnarray}$ Z检验（T检验的变体）Z检验并不能算是一种全新的统计检验方法，刚刚的T检验，我们是只知道总体的均值$\mu$,但不知道总体的方差。如果我们既知道总体的均值，也知道总体的方差$\sigma$，那此时就适用Z检验。 此时Z-score统计量公式为： $$Z= \frac{\hat{x}-1.1}{\sigma/ \sqrt{n}}$$ Z检验就是查正态分布表。因为Z-score是完全服从$N(0,1)$正态分布，事实上，当n的数量足够大时（一般&gt;45)，T检验都可以不用查表了，直接查正态分布表即可。 $\chi$检验又称Chi-Squared Test，最有名的是皮尔逊卡方检验，维基百科对此有很好的描述。 皮尔森卡方检验（英语：Pearson’s chi-squared test）是最有名卡方检验之一（其他常用的卡方检验还有叶氏连续性校正、似然比检验、一元混成检验等等－－它们的统计值之机率分配都近似于卡方分配，故称卡方检验） “皮尔森卡方检验”的虚无假设（H0）是：一个样本中已发生事件的次数分配会遵守某个特定的理论分配。 在虚无假设的句子中，“事件”必须互斥，并且所有事件总机率等于1。或者说，每个事件是类别变量（英语：categorical variable）的一种类别或级别（英语：level）。 简单的例子：常见的六面骰子，事件＝丢骰子的结果（可能是1~6任一个）属于类别变量，每一面都是此变量的一种（一个级别）结果，每种结果互斥（1不是2, 3, 4, 5, 6; 2不是1, 3, 4 …），六面的机率总和等于1。 卡方检验的一般步骤： “皮尔森卡方检验”可用于两种情境的变项比较：适配度检验，和独立性检验。 “适配度检验”验证一组观察值的次数分配是否异于理论上的分配。 “独立性检验”验证从两个变量抽出的配对观察值组是否互相独立（例如：每次都从A国和B国各抽一个人，看他们的反应是否与国籍无关）。 不管哪个检验都包含三个步骤： 计算卡方检验的统计值“ $\chi ^{2}$ ”：把每一个观察值和理论值的差做平方后、除以理论值、再加总。 计算 $\chi ^{2}$ 统计值的自由度“ $df$”。 依据研究者设定的置信水准，查出自由度为 $df$ 的卡方分配临界值，比较它与第1步骤得出的 $\chi ^{2}$ 统计值，推论能否拒绝虚无假设。 F检验https://zh.wikipedia.org/wiki/F%E6%A3%80%E9%AA%8C 通常的F检验例子包括： 假设一系列服从正态分布的母体，都有相同的标准差。这是最典型的F检验，该检验在方差分析（ANOVA）中也非常重要。 假设一个回归模型很好地符合其数据集要求。 F检验对于数据的正态性非常敏感，因此在检验方差齐性的时候，Levene检验, Bartlett检验或者Brown–Forsythe检验的稳健性都要优于F检验。 F检验还可以用于三组或者多组之间的均值比较，但是如果被检验的数据无法满足均是正态分布的条件时，该数据的稳健型会大打折扣，特别是当显著性水平比较低时。但是，如果数据符合正态分布，而且alpha值至少为0.05，该检验的稳健型还是相当可靠的。 F检验又叫方差齐性检验，用于检验两组服从正态分布的样本是否具有相同的总体方差，即方差齐性。 F检验步骤​ 假设两组服从不同正态分布的数据样本 (1) $\begin{eqnarray} \pmb{x}:&amp;1,\quad 2,\quad 3,\quad 1,\quad 2 \nonumber \ \pmb{y}:&amp;1,\quad 3,\quad 2,\quad 2 \nonumber \end{eqnarray}$ 我们使用F检验检查这两组数据的总体方差是否相等。F 检验的主要步骤如下: 步骤1. 设$(\pmb{x} \sim N(u_1,\sigma^2_1))以及(\pmb{y} \sim N(u_2,\sigma^2_2))$。建立零假设$(\pmb{H}_0)$和备选假设$(\pmb{H}_1)$。 步骤2. 我们选择一个合适统计量$(s = \frac{s_x^2}{s_y^2})$，其中$(s_x^2)$为$(\pmb{x})$的样本方差，$(s_y^2)$为$(\pmb{y})$的样本方差。 步骤3. 查F检验临界值表。样本$(\pmb{x})$有5个数据，因此我们采用分子自由度为4所对应的列；样本$(\pmb{y})$有4个数据，因此我们采用分母自由度为3所对应的大行；显著性水平为$(\alpha=0.10)$，我们采用$(p=0.10)$所对应的小行。 查表所得值为28.71。(s = 0.972 &lt; 28.71)，故我们接受零假设，认为$(\pmb{H}_0: \sigma_1^2 = \sigma_2^2 )$。 总结1，t检验 ​ 单总体方差未知，检验单总体均值是否= or &gt;= or &lt;= $\mu_0$; ​ 两总体方差未知，检验两总体均值是否= or &gt;= or &lt;=，是否相互独立导致统计量不同; 2，卡方检验 ​ 检验单总体方差是否= or &gt;= or &lt;= $\sigma_0$; 3，F检验 ​ 检验两相互独立总体方差是否= or &gt;= or &lt;=； 相应的统计量都很容易找到，注意适用条件!!]]></content>
      <tags>
        <tag>Statistics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何读懂腰椎MRI核磁片]]></title>
    <url>%2F2017%2F171006_%E5%A6%82%E4%BD%95%E8%AF%BB%E6%87%82%E8%85%B0%E6%A4%8EMRI%E6%A0%B8%E7%A3%81%E7%89%87%2F</url>
    <content type="text"><![CDATA[如何读懂腰椎MRI核磁片作者：Dr. Douglas M. Gillard DC 翻译：Daniel 本文是由本博客作者翻译自Dr,Douglas，有条件的读者建议直接去看英文解释可能会更有收获。因为很多程序员都有腰椎的问题，我自己也是有一些小问题，在拍了MRI后，除了被医生敷衍几句，可能得不到更多有效的信息。这篇文章的英文原文是国内很多如何读懂MRI相关教程的来源，但我觉得他们翻译的并不是很好，有些地方会让新手产生困惑，因此我自己再翻译了一遍，希望可以帮助到一些人。 注意：在阅读本文之前，你需要了解到MRI的结果可能只能解释疼痛原因的一小部分，并且在没有体检结果，病史，多年经验的医生的指导下，MRI结果是完全无用的。所以，千万不要尝试用在这里学到的信息去揣测你的医生。请记住：在涉及椎间盘突出病例中，MRI具有至少30%的假阳性率。所以，仅仅因为你有一个椎间盘突出了，并不意味着这是你疼痛的原因。 1.如何定位椎间盘：定位像定位像非常重要，它告诉了我们哪一个椎间盘轴向图像（椎间盘俯视图）对应哪一个椎间盘。 例如图1.1的定位像中，编号10的直线告诉我们这个位置的切片是10号轴向视图。这个切片的位置正好位于L4/L5椎间盘之间的底部。 MRI医生有专业的查看软件，可以在定位像上任意移动鼠标，同时显示对应位置的轴向和矢状面（侧面）视图。 图1.1 2.轴向视图图2.1是一张T1加权成像的轴向L5/S1椎间盘MRI图像。左图和右图的区别仅在于右图有一些额外的彩色标记，来帮助我们了解各个结构，在学习之后，你可以通过观察左侧图像来练习。 图2.1我们的椎间盘是橄榄球形的结构（黄色轮廓线），橙色描绘的像弹弓的一样的结构是我们的后弓。后弓由棘突（spinous process， 图中SP)，椎板(lamina, 图中LAM)和小关节(facet)组成，而小关节本身由两部分组成：骶骨的上关节突起（SAP，因为这是L5/S1椎间盘，S1就是骶骨)和L5的下关节突起。在后弓这个弹弓结构的怀抱中，就是椎管，内部包裹着神经组织，看起来有点像米老鼠的头。米老鼠的头部（红色轮廓线）是硬膜囊，保护着内部的下垂的神经根，硬膜囊内充满了脑脊液；米老鼠的两个耳朵（绿色轮廓线）是贯穿神经根（trasversing nerve roots, 译者注：我理解为贯穿神经根会待继续在椎管内，暂时还不会伸出脊柱，对应的概念是exiting nerve roots, 发出神经根，就是从该节神经孔中出来的神经根，具体可以参看图2.3中神经根从椎管离开 ）跳进骶骨骶孔，然后从下一节骶骨椎间孔退出。紫红色轮廓线圈住的是可见的L5发出神经根，因为他们是发出神经根，你只能看到背部神经节，这些神经节通过叫做椎间孔的结果离开脊柱（见图2.3）。 绿色圆圈所示的贯穿神经根是在侧隐窝区域（图中未标注），这里是椎间盘突出最常见部位。 T1加权像不能显示组织的水分，因此，我们无法看到硬膜囊内的脑脊液，和椎间盘的髓核含水情况。 图2.2这张图展示了一名45岁男性健康L4/L5椎间盘的T2加权轴像。由于T2加权像能够显示含水量，我们可以明显看到椎间盘的中心的髓核（浅色中心），外围是颜色较暗的纤维环。我们也可以清楚地看到硬膜囊内的神经根，注意他们是怎么排列的。其他结构都如图2.1所示。这是一个非常健康的椎间盘，很多时候，由于失去水分，你可能会看不到浅色的髓核。 图2.3 3. 侧面视图图3.1是腰椎T2加权像的侧面视图，或称为矢状面视图。 首先看一些基本结构，椎间盘，位于两个椎骨之间，内部应为白色（含水），在这张图中，我们看到L5/S1椎间盘有黑化迹象（脱水），这代表有轻度的椎间盘退变性疾病。 图中蓝色小箭头指向的是后纵韧带，在影像中显示为一条在椎体和椎间盘后方向下运行的黑色线条。神奇的是，尽管这张图的患者有9毫米的突出，后纵韧带仍然包裹住了突出物，且没有破损，但显然后纵韧带受到突出物挤压，一部分从椎骨表面脱离，这种情况被称为包含性椎间盘脱出。 硬膜囊在图中是一种“超白色”结构，位于中央椎管。内部包含了自由浮动的脊神经根（马尾），包括运动和感觉神经纤维。 绿色星星指向的是黄韧带，它位于每个椎骨之间，增加脊柱的稳定性。这种结构增大/增厚会引起可怕的椎管狭窄，常见于老年患者。 图3.2是T2加权像侧面视图，与图3.1不同的是，它的侧面切面位置在边缘，因此它能展示出非常重要的神经孔和位于其中的发出神经根。这是非常重要的切片，医生应该在两个侧边都要进行仔细查看，以确保这里没有发生椎间盘突出。通常来说，椎间盘突出不会在这个区域发生，但是如果是这种情况，会引起非常严重的坐骨神经痛，手术也有较高难度。 图3.2 4. 例子图4.1是一个真实病例，左侧是L5/S1的轴向视图，右侧是L5/S1的侧面视图，并且这是T1加权像。 图4.1红色星星所在位置是一个9毫米大的突出物。注意，这个突出物已经将患者右侧的S1贯穿神经根（图中左侧）完全遮住了，并将其压迫到椎板上（绿色小箭头）。同时硬膜囊被这个突出物也严重挤压，这在轴向和侧面视图中都能看得很清楚。 这是个24岁的年轻人，他最终选择保守治疗，没有进行手术，最终恢复的不错，但是他注定是和繁重体力劳动无缘了。]]></content>
      <tags>
        <tag>Health</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习ReLearn Part1]]></title>
    <url>%2F2017%2F170628_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%80%2F</url>
    <content type="text"><![CDATA[机器学习-第一章 绪论 1.4 归纳偏好 机器学习算法在学习过程中对某种类型假设的偏好，称为“归纳偏好”（inductive bias)。归纳偏好可以看做学习算法在一个可能很庞大的假设空间中对假设进行选择的启发式或者“价值观”。有没有一般性原则来引导算法确立“正确”的偏好呢？那就是“奥卡姆剃刀”原则：若有多个假设与观察一致，选择最简单的那个。 No free lunch 理论 无论一种学习算法$$L_a$$多么聪明，学习算法$$L_b$$多么笨拙，它们在整个假设空间中的期望性能是相同的。 就像这张图展示的，A曲线应该更合理，更符合奥卡姆剃刀原则，但你不能否认另一种归纳偏好与B曲线更好符合的存在。这样来看，聪明的算法与随机乱猜算法，如果考虑所有的归纳偏好的可能性，他们的性能是一样的。 但NFL定理又一个重要前提：所有问题出现的机会相同、或所有问题同等重要。但实际情形并非这样，大部分时候，我们只关注自己正在试图解决的问题。所以NFL最重要的寓意是让我们清楚认识到，脱离具体问题，空泛地谈论“什么学习算法更好”毫无意义，因为若考虑所有潜在的问题，则所有学习算法都一样好。要谈论算法的优劣，必须要针对具体的学习问题。]]></content>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux文本编辑器介绍与使用]]></title>
    <url>%2F2017%2F180401_%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91%2F</url>
    <content type="text"><![CDATA[VimVim的命名： Vi +IMproved Vim的使用可以大致分为两种模式：命令行模式（command mode)和插入模式(insert mode)。 i切换进入插入模式，类似的，也可以使用a,o进入插入模式，只是插入的点位有区别。 在命令行模式下： 删除文字： x每按一次，删除光标所在位置后面一个字符 6x,删除光标所在位置后面6个字符 X,删除光标所在位置前面一个字符 dd，删除光标所在行 6dd，从光标所在行开始删除6行 复制 复制和y有关，并且都要搭配p命令才能完成粘贴 yw 将光标所在之处到字尾的字符复制到缓冲区 6yw复制6个字符到缓冲区 yy复制光标所在行到缓冲区 6yy复制光标下面6行文字到缓冲区 p将缓冲区内的字符贴到光标所在位置 撤销 u 撤销上一个操作，可以连续撤销 跳至制定行 ctrl+g 列出光标所在行号 15G 移动光标至第15行行首 last line mode 进入last line mode,先确保处在命令行模式，然后按:即可进入` :set nu 列出行号 :15 跳至15行 :/keyword 查找keyword，可以一直按n跳至下一个匹配 :w 保存文件 :qw 保存并退出vim SEDsed（stream editor)是一个批处理（非交互式）编辑器，它可以变换来自文件或标准输入的输入流，常用作管道中的过滤器。由于sed仅仅对输入扫描一次，因此它比其他的交互式编辑器更加高效。 sed命令行语法： sed [-n] program [file-list] 比如说，将lines这个文件中出现line的行都打印出来： cat lines Line one. The second line. Third. Line four. sed -n &#39;/line/ p&#39; lines The second line. Line four. 上述命令中，/line/是字符串的正则表达式， p显示选定的行, -n表示仅仅显示选定的行，如果不加-n，所有行都会被输出到标准输出，并且被选中的行会输出两遍。 sed program a(append) sed &#39;2a after&#39; lines 在第二行之后增加’after’ i(insert) sed &#39;2i before&#39; lines 在第二行之前增加’before’ p(print) r(read) file sed &#39;2r file_name&#39; lines 在第二行之后挂上file_name里的内容。 AWKawk是一种模式扫描和处理语言，由它的三个作者姓名的首字母命名。它搜索一个或者多个文件，已查看这些文件中是否存在匹配指定模式的记录（通常是文本行）。每次发现匹配的记录时，它通过执行动作的方式（比如将该记录写到标准输出或者将某个计数器递增）来处理文本行。与过程语言相反，AWK属于数据驱动语言：用户描述想要处理的数据并告诉AWK当他发现这些数据时如何处理它们。 AWK用法和SED很像，直接看例子吧。 cat cars plym fury 1970 73 2500 chevy malibu 1999 60 3000 ford mustang 1965 45 10000 volvo s80 1998 102 9850 chevy malibu 2000 50 3500 bmw 325i 1985 115 450 gawk &#39;{print }&#39; cars plym fury 1970 73 2500 chevy malibu 1999 60 3000 ford mustang 1965 45 10000 … gawk &#39;/chevy/&#39; cars chevy malibu 1999 60 3000 chevy malibu 2000 50 3500 gawk &#39;/chevy/ {print $3, $1}&#39; cars 选中包含字符串’chevy’的所有行并显示选中行的第三个字段和第一个字段 1999 chevy 2000 chevy gawk &#39;$5 &lt;= 3000&#39; cars plym fury 1970 73 2500 chevy malibu 1999 60 3000 chevy malibu 2000 50 3500 bmw 325i 1985 115 450 CAT连接并显示文件，将文件复制到标准输出。可以使用cat在屏幕上显示一个或者多个文本文件内容。 cat [options] [file_list] file_list为cat要处理的一个或者多个文件的路径名列表。如果不指定任何参数，或者指定一个连字符（-）代替文件名，cat就从标准输入读取输入信息。 cat file1 file2 &gt; output_file 将file1和file2合并为output_file 在不使用编辑器的情况下，可以使用cat创建较短的文本文件。 cat &gt; new_file 从标准输入输入内容 Ctrl+D 结束输入(发出EOF信号) who | cat header - footer &gt; output 管道将who的输出发送到cat的标准输入，shell将cat的输出重定向到文件output中，output文件将包括header,who输出结果，和footer。]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop与Spark区别和联系]]></title>
    <url>%2F2017%2F170626_Hadoop%E4%B8%8ESpark%E5%8C%BA%E5%88%AB%E5%92%8C%E8%81%94%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[公司的集群用的是spark系统，但一直就是这么稀里糊涂的用着，这里只是简单梳理一下各种名称的含义和背后的关系。 Hadoop 1.0是由分布式存储系统HDFS和分布式计算框架MapReduce组成， Hadoop 2.0在此基础，针对MapReduce在扩展性和多框架支持方面的不足，提出了全新的资源管理框架YARN(yet another resource negotiator) YARN是在Hadoop MapReduce基础上演化而来的，在MapReduce时代，很多人批评MapReduce不适合迭代计算和流失计算，于是出现了Spark和Storm等计算框架，而这些系统的开发者则在自己的网站上或者论文里与MapReduce对比，鼓吹自己的系统多么先进高效，而出现了YARN之后，则形势变得明朗：MapReduce只是运行在YARN之上的一类应用程序抽象，Spark和Storm本质上也是，他们只是针对不同类型的应用开发的，没有优劣之别，各有所长，合并共处，而且，今后所有计算框架的开发，不出意外的话，也应是在YARN之上。这样，一个以YARN为底层资源管理平台，多种计算框架运行于其上的生态系统诞生了。 Spark只是一种内存计算框架，用Scala编写，它并没有自己的文件管理系统，所以Spark一般是依附于Hadoop的HDFS。可以将Spark理解为带cache的MapReduce框架。你看Spark官网的评价： Spark is a MapReduce-like cluster computing framework designed for low-latency iterative jobs and interactive use from an interpreter. Hive是Cloudera发起的项目，一开始是为了将sql语言转换为mapreduce流程所设计，但后来开发了hive on spark，就能将Spark作为Hive的计算引擎，用类SQL语言实现计算，称为H(ive)QL。但Spark本身自己也有一套SQL查询模块，叫做Spark-sql。 SparkSQL和Hive On Spark都是在Spark上实现SQL的解决方案。Spark早先有Shark项目用来实现SQL层，不过后来推翻重做了，就变成了SparkSQL。这是Spark官方Databricks的项目，Spark项目本身主推的SQL实现。Hive On Spark比SparkSQL稍晚。Hive原本是没有很好支持MapReduce之外的引擎的，而Hive On Tez项目让Hive得以支持和Spark近似的Planning结构（非MapReduce的DAG）。所以在此基础上，Cloudera主导启动了Hive On Spark。这个项目得到了IBM，Intel和MapR的支持（但是没有Databricks）。 需要理解的是，Hive和SparkSQL都不负责计算，它们只是告诉Spark，你需要这样算那样算，但是本身并不直接参与计算。 Hbase： Hadoop database 的简称，也就是基于Hadoop数据库，是一种NoSQL数据库 一个数据仓库的架构基本就是：底层HDFS，上面跑MapReduce/Tez/Spark，在上面跑Hive，Pig。 HBase is an open-source, distributed, column-oriented database built on top of HDFS (or KFS) based on BigTable!]]></content>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
</search>
